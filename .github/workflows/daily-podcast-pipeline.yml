name: Daily Tech Digest Pipeline

on:
  schedule:
    # Run Monday-Friday at 6 AM UTC (weekdays only)
    - cron: '0 6 * * 1-5'
  workflow_dispatch:  # Allow manual triggering
    inputs:
      mode:
        description: 'Processing mode'
        required: false
        default: 'normal'
        type: choice
        options:
        - normal
        - test-run
        - friday-weekly
        - monday-catchup
      debug:
        description: 'Enable debug output'
        required: false
        default: false
        type: boolean

permissions:
  contents: write  # Allow the workflow to push changes
  packages: read   # Allow reading packages if needed

# Prevent overlapping runs
concurrency:
  group: daily-digest-${{ github.ref }}
  cancel-in-progress: false

jobs:
  smoke-test:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    env:
      TZ: UTC
      PYTHONUNBUFFERED: "1"
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        
    - name: Cache Python dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}-v1
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        
    - name: Cache system dependencies  
      uses: awalsh128/cache-apt-pkgs-action@latest
      with:
        packages: ffmpeg sqlite3
        version: 1.0
        
    - name: Check for direct sqlite3.connect usage
      run: |
        echo "ðŸ” Checking for direct sqlite3.connect usage..."
        if find . -name "*.py" -not -path "./utils/db.py" -not -path "./.github/*" -exec grep -l "sqlite3\.connect" {} \; | head -1 | grep -q .; then
          echo "âŒ ERROR: Direct sqlite3.connect found! Use utils.db.get_connection()"
          find . -name "*.py" -not -path "./utils/db.py" -not -path "./.github/*" -exec grep -Hn "sqlite3\.connect" {} \;
          exit 1
        else
          echo "âœ… No direct sqlite3.connect usage found"
        fi
    
    - name: Run schema integrity verification
      run: |
        echo "ðŸ” Running schema integrity checks..."
        # Skip schema version validation for CI (databases may not be migrated yet)
        if [ -f "podcast_monitor.db" ]; then
          python scripts/verify_schema_integrity.py podcast_monitor.db --output schema_integrity_report.json || echo "Schema checks failed but continuing..."
        else
          echo "Database not present in CI, skipping integrity check"
        fi
    
    - name: Run CI smoke tests
      run: |
        echo "ðŸ§ª Running fast CI smoke tests..."
        ./scripts/ci_smoke.sh

  daily-podcast-digest:
    runs-on: ubuntu-latest
    timeout-minutes: 90
    needs: smoke-test
    env:
      TZ: UTC
      PYTHONUNBUFFERED: "1"
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch full history for proper git operations
        token: ${{ secrets.GITHUB_TOKEN }}
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        
    - name: Cache Python dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}-v1
        restore-keys: |
          ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}-
          ${{ runner.os }}-pip-
          
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        # sqlite3 is built into Python, no need to install via pip
        
    - name: Cache system dependencies  
      uses: awalsh128/cache-apt-pkgs-action@latest
      with:
        packages: ffmpeg sqlite3
        version: 1.0
        
    # Note: Removed database caching - databases should use current repo state, not cached versions
    # This allows database changes committed to the repo to be properly used by the workflow
          
    - name: Show workflow configuration  
      run: |
        echo "ðŸŽ¯ Workflow Mode: ${{ github.event.inputs.mode || 'normal (scheduled)' }}"
        echo "ðŸ› Debug Mode: ${{ github.event.inputs.debug || 'false' }}"
        echo "âš¡ Trigger: ${{ github.event_name }}"
        echo "ðŸ“… Current day: $(date -u +%A)"
        
        # Determine workflow behavior based on day and mode
        WORKFLOW_MODE="${{ github.event.inputs.mode }}"
        CURRENT_DAY=$(date -u +%u)  # 1=Monday, 5=Friday
        
        if [ "$WORKFLOW_MODE" == "test-run" ]; then
          echo "ðŸ§ª TEST MODE: No commits, pushes, or releases will be made"
        elif [ "$WORKFLOW_MODE" == "friday-weekly" ] || [ "$CURRENT_DAY" == "5" ]; then
          echo "ðŸ—“ï¸  FRIDAY MODE: Daily + Weekly digest generation"
          echo "FRIDAY_WEEKLY=true" >> $GITHUB_ENV
        elif [ "$WORKFLOW_MODE" == "monday-catchup" ] || [ "$CURRENT_DAY" == "1" ]; then
          echo "ðŸ“… MONDAY MODE: Catch-up processing since Friday 06:00"
          echo "MONDAY_CATCHUP=true" >> $GITHUB_ENV
        else
          echo "ðŸ“† REGULAR WEEKDAY: Standard daily processing"
        fi
        
    - name: Fast-fail environment validation
      timeout-minutes: 2
      env:
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ELEVENLABS_API_KEY: ${{ secrets.ELEVENLABS_API_KEY }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        HEALTHCHECK_URL_RSS: ${{ secrets.HEALTHCHECK_URL_RSS }}
        HEALTHCHECK_URL_YT: ${{ secrets.HEALTHCHECK_URL_YT }}
      run: |
        echo "ðŸ” Fast-fail environment validation"
        
        # Critical secrets validation
        if [ -z "$ANTHROPIC_API_KEY" ]; then
          echo "âŒ ANTHROPIC_API_KEY secret is required but not set"
          exit 1
        fi
        
        if [ -z "$GITHUB_TOKEN" ]; then
          echo "âŒ GITHUB_TOKEN secret is required but not set"
          exit 1
        fi
        
        # Optional secrets validation with warnings
        if [ -z "$OPENAI_API_KEY" ]; then
          echo "âš ï¸ OPENAI_API_KEY not set - topic scoring will be skipped"
        fi
        
        if [ -z "$ELEVENLABS_API_KEY" ]; then
          echo "âš ï¸ ELEVENLABS_API_KEY not set - TTS will be skipped"
        fi
        
        if [ -z "$HEALTHCHECK_URL_RSS" ]; then
          echo "âš ï¸ HEALTHCHECK_URL_RSS not set - RSS healthcheck disabled"
        fi
        
        if [ -z "$HEALTHCHECK_URL_YT" ]; then
          echo "âš ï¸ HEALTHCHECK_URL_YT not set - YouTube healthcheck disabled"
        fi
        
        echo "âœ… Environment validation passed"
        
        # Set environment variables for subsequent steps
        echo "ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY" >> $GITHUB_ENV
        echo "OPENAI_API_KEY=$OPENAI_API_KEY" >> $GITHUB_ENV
        echo "ELEVENLABS_API_KEY=$ELEVENLABS_API_KEY" >> $GITHUB_ENV
        echo "GITHUB_TOKEN=$GITHUB_TOKEN" >> $GITHUB_ENV
        echo "HEALTHCHECK_URL_RSS=$HEALTHCHECK_URL_RSS" >> $GITHUB_ENV
        echo "HEALTHCHECK_URL_YT=$HEALTHCHECK_URL_YT" >> $GITHUB_ENV
        
        # Set podcast base URL for deployment
        echo "PODCAST_BASE_URL=https://podcast.paulrbrown.org" >> $GITHUB_ENV
        
    - name: Read YouTube watermark for traceability
      timeout-minutes: 1
      run: |
        echo "ðŸ“„ Reading YouTube watermark for traceability"
        
        if [ -f "state/yt_last_push.json" ]; then
          echo "âœ… YouTube watermark file found"
          
          # Parse watermark for key info
          RUN_ID=$(jq -r '.run_id // "unknown"' state/yt_last_push.json)
          COMMIT_SHA=$(jq -r '.commit_sha // "unknown"' state/yt_last_push.json)
          PUSHED_AT=$(jq -r '.pushed_at // "unknown"' state/yt_last_push.json)
          STATUS=$(jq -r '.status // "unknown"' state/yt_last_push.json)
          EPISODES=$(jq -r '.episodes_processed // 0' state/yt_last_push.json)
          
          echo "ðŸŽ¯ Last YouTube sync:"
          echo "  Run ID: $RUN_ID"
          echo "  Commit: ${COMMIT_SHA:0:8}"
          echo "  Pushed at: $PUSHED_AT"
          echo "  Status: $STATUS"
          echo "  Episodes: $EPISODES"
          
          # Check if watermark is stale (>8 hours old)
          if command -v date >/dev/null 2>&1; then
            if [ "$PUSHED_AT" != "unknown" ] && [ "$PUSHED_AT" != "" ]; then
              PUSHED_EPOCH=$(date -d "$PUSHED_AT" +%s 2>/dev/null || echo "0")
              NOW_EPOCH=$(date +%s)
              AGE_HOURS=$(( (NOW_EPOCH - PUSHED_EPOCH) / 3600 ))
              
              if [ $AGE_HOURS -gt 8 ]; then
                echo "âš ï¸ YouTube watermark is stale (${AGE_HOURS} hours old)"
                echo "STALE_WATERMARK=true" >> $GITHUB_ENV
              else
                echo "âœ… YouTube watermark is fresh (${AGE_HOURS} hours old)"
              fi
            fi
          fi
          
          # Add to job summary
          echo "## ðŸ“„ YouTube Sync Status" >> $GITHUB_STEP_SUMMARY
          echo "- **Last Run ID:** \`$RUN_ID\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit SHA:** \`${COMMIT_SHA:0:8}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Pushed:** $PUSHED_AT" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** $STATUS" >> $GITHUB_STEP_SUMMARY
          echo "- **Episodes Processed:** $EPISODES" >> $GITHUB_STEP_SUMMARY
          
        else
          echo "âš ï¸ YouTube watermark file not found - first run or missing state"
          echo "MISSING_WATERMARK=true" >> $GITHUB_ENV
          
          echo "## âš ï¸ YouTube Sync Status" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** Missing watermark file" >> $GITHUB_STEP_SUMMARY
          echo "- **Note:** This may be the first run or the YouTube cron job hasn't pushed state yet" >> $GITHUB_STEP_SUMMARY
        fi
        
    - name: Validate critical configuration
      run: |
        echo "ðŸ” Validating critical configuration before pipeline..."
        python config_validator.py --all
        
    - name: Run Multi-Topic Podcast Pipeline (GitHub Actions mode)
      run: |
        if [ "${{ github.event.inputs.debug }}" == "true" ]; then
          echo "ðŸ› Debug mode enabled"
          export DEBUG=1
        fi
        
        if [ "${{ github.event.inputs.mode }}" == "test-run" ]; then
          echo "ðŸ§ª Test run mode - will not deploy or update feeds"
          python3 daily_podcast_pipeline.py --rss-only --test
        elif [ "$FRIDAY_WEEKLY" == "true" ]; then
          echo "ðŸ—“ï¸  Friday mode - generating daily + weekly digests"
          python3 daily_podcast_pipeline.py --rss-only
        elif [ "$MONDAY_CATCHUP" == "true" ]; then
          echo "ðŸ“… Monday catch-up mode - processing since Friday"
          python3 daily_podcast_pipeline.py --rss-only
        else
          echo "ðŸš€ Regular weekday processing mode"
          python3 daily_podcast_pipeline.py --rss-only
        fi
        
    - name: Debug TTS Generation (CRITICAL)
      if: always()
      run: |
        echo "=========================================="
        echo "ðŸŽµ TTS GENERATION DEBUGGING (CRITICAL)"
        echo "=========================================="
        
        echo "=== Environment Variables ==="
        echo "ELEVENLABS_API_KEY length: ${#ELEVENLABS_API_KEY}"
        echo "ANTHROPIC_API_KEY length: ${#ANTHROPIC_API_KEY}"
        if [ -z "$ELEVENLABS_API_KEY" ]; then
          echo "âŒ ELEVENLABS_API_KEY is EMPTY or UNSET"
        else
          echo "âœ… ELEVENLABS_API_KEY is set"
        fi
        
        echo "=== Daily Digests Directory ==="
        ls -la daily_digests/ || echo "âŒ No daily_digests/ directory"
        
        echo "=== Topic-Specific Digest Files ==="
        find daily_digests/ -name "*_digest_*.md" -exec echo "ðŸ“„ Found digest: {}" \; 2>/dev/null || echo "âŒ No topic digest files found"
        
        echo "=== Topic-Specific Audio Files (Enhanced and Standard) ==="
        find daily_digests/ -name "*_digest_*_enhanced.mp3" -exec echo "ðŸŽµ Found enhanced audio: {}" \; 2>/dev/null || echo "â„¹ï¸  No enhanced audio files found"
        find daily_digests/ -name "*_digest_*.mp3" ! -name "*_enhanced.mp3" -exec echo "ðŸŽµ Found standard audio: {}" \; 2>/dev/null || echo "âŒ No standard audio files found"
        
        echo "=== All TTS Text Files ==="
        find daily_digests/ -name "*_digest_tts_*.txt" -exec echo "ðŸ“ Found TTS text: {}" \; 2>/dev/null || echo "âš ï¸ No TTS text files found"
        
        echo "=== All JSON Metadata Files ==="
        find daily_digests/ -name "*_digest_*.json" -exec echo "ðŸ“Š Found metadata: {}" \; 2>/dev/null || echo "âš ï¸ No metadata files found"
        
        echo "=== Manual Multi-Topic TTS Generation Test ==="
        echo "ðŸ§ª Testing multi-topic TTS script directly..."
        python3 multi_topic_tts_generator.py 2>&1 || echo "âŒ Multi-topic TTS script failed"
        
        echo "=== Post-TTS Directory Check ==="
        ls -la daily_digests/ | grep -E "\.(mp3|json|txt|md)$" || echo "âŒ No files after TTS test"
        
        echo "=== TTS Script Output Analysis ==="
        if ls daily_digests/claude_digest_full_*.txt 1> /dev/null 2>&1; then
          echo "âœ… TTS full script found"
        else
          echo "âŒ TTS full script missing"
        fi
        
        if ls daily_digests/claude_digest_tts_*.txt 1> /dev/null 2>&1; then
          echo "âœ… TTS optimized script found"  
        else
          echo "âŒ TTS optimized script missing"
        fi
        
        echo "=== Topic Digest Timestamp Matching ==="
        LATEST_DIGEST=$(find daily_digests/ -name "*_digest_*.md" -printf '%T@ %p\n' 2>/dev/null | sort -n | tail -1 | cut -d' ' -f2- || echo "")
        if [ ! -z "$LATEST_DIGEST" ] && [ -f "$LATEST_DIGEST" ]; then
          DIGEST_BASENAME=$(basename "$LATEST_DIGEST" .md)
          echo "ðŸ“„ Latest digest file: $DIGEST_BASENAME"
          EXPECTED_AUDIO="daily_digests/${DIGEST_BASENAME}.mp3"
          echo "ðŸŽ¯ Expected audio file: $EXPECTED_AUDIO"
          if [ -f "$EXPECTED_AUDIO" ]; then
            echo "âœ… MATCHING AUDIO FILE FOUND!"
            ls -la "$EXPECTED_AUDIO"
          else
            echo "âŒ MATCHING AUDIO FILE MISSING!"
            echo "ðŸ” Checking for all audio files with similar timestamp..."
            TIMESTAMP_PATTERN=$(echo "$DIGEST_BASENAME" | grep -o '[0-9]\{8\}_[0-9]\{6\}' || echo "")
            if [ ! -z "$TIMESTAMP_PATTERN" ]; then
              find daily_digests/ -name "*${TIMESTAMP_PATTERN}*" -type f 2>/dev/null || echo "No files found with timestamp $TIMESTAMP_PATTERN"
            fi
          fi
        else
          echo "âŒ No digest file found for timestamp matching"
        fi
        
        echo "=========================================="
        
    - name: Debug transcript status
      if: always()
      run: |
        echo "=== Transcript Directory Status ==="
        ls -la transcripts/ || echo "No transcripts/ directory"
        echo "=== Digested Transcripts ==="
        ls -la transcripts/digested/ || echo "No transcripts/digested/ directory"
        echo "=== RSS Database Status ==="
        sqlite3 podcast_monitor.db "SELECT status, COUNT(*) as count FROM episodes GROUP BY status;" || echo "RSS database query failed"
        echo "=== YouTube Database Status ==="
        sqlite3 youtube_transcripts.db "SELECT status, COUNT(*) as count FROM episodes GROUP BY status;" || echo "YouTube database query failed or not found"
        
    - name: Upload generated artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: podcast-digest-${{ github.run_number }}
        path: |
          daily_digests/
          transcripts/
          transcripts/digested/
          podcast_monitor.db
          youtube_transcripts.db
        retention-days: 30
        
    - name: Commit and push changes
      if: success() && github.event.inputs.mode != 'test-run'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Add all changes including both databases
        git add podcast_monitor.db youtube_transcripts.db
        git add daily_digests/
        git add transcripts/
        git add transcripts/digested/
        git add daily-digest.xml
        
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          # Create detailed commit message
          git commit -m "Daily podcast digest update - $(date -u +%Y-%m-%d)
          
          RSS episodes processed and YouTube episode statuses synced
          - Updated podcast_monitor.db (RSS episodes marked as digested)
          - Updated youtube_transcripts.db (YouTube episodes marked as digested)
          - Generated daily digest and TTS audio
          - Updated RSS feed"
          git push
          echo "âœ… Committed digest updates to both databases"
        fi
        
    - name: Deploy multi-topic episodes (using new deployment system)
      if: success() && hashFiles('daily_digests/*_digest_*.md') != '' && github.event.inputs.mode != 'test-run'
      run: |
        echo "ðŸš€ Using new multi-topic deployment system..."
        python3 deploy_multi_topic.py
        
    - name: Update RSS feed (using multi-topic generator)
      if: success() && github.event.inputs.mode != 'test-run'
      timeout-minutes: 10
      run: |
        echo "ðŸ“¡ Generating RSS feed with multi-topic support..."
        python3 rss_generator_multi_topic.py
        
        # Commit RSS feed changes
        if [ -f "daily-digest.xml" ]; then
          git add daily-digest.xml
          if ! git diff --staged --quiet -- daily-digest.xml; then
            git commit -m "Update RSS feed with multi-topic digest episodes - $(date -u +%Y-%m-%d)" daily-digest.xml || echo "Failed to commit RSS feed"
            git push || echo "Failed to push RSS feed"
          fi
        fi
    
    - name: Send success healthcheck ping
      if: success() && github.event.inputs.mode != 'test-run'
      timeout-minutes: 2
      run: |
        if [ -n "${HEALTHCHECK_URL_RSS:-}" ]; then
          echo "ðŸ“Š Sending success healthcheck ping"
          
          # Count episodes processed (estimate from digest files)
          EPISODES_COUNT=0
          if [ -d "daily_digests" ]; then
            EPISODES_COUNT=$(find daily_digests -name "*_digest_*.md" -type f | wc -l | tr -d ' ')
          fi
          
          # Calculate workflow duration (approximate)
          WORKFLOW_DURATION=$(( $(date +%s) - $(date -d "${{ github.event.created_at }}" +%s 2>/dev/null || echo $(date +%s)) ))
          
          # Create healthcheck payload
          PAYLOAD=$(jq -n \
            --arg episodes "$EPISODES_COUNT" \
            --arg duration "$WORKFLOW_DURATION" \
            --arg status "success" \
            --arg run_id "${{ github.run_id }}" \
            '{
              episodes_processed: ($episodes | tonumber),
              duration_seconds: ($duration | tonumber),
              status: $status,
              github_run_id: $run_id
            }')
          
          if curl -m 10 -fsS "$HEALTHCHECK_URL_RSS" \
            -H "Content-Type: application/json" \
            -d "$PAYLOAD" >/dev/null 2>&1; then
            echo "âœ… Healthcheck ping sent successfully"
          else
            echo "âš ï¸ Failed to send healthcheck ping (non-blocking)"
          fi
        else
          echo "â„¹ï¸ No healthcheck URL configured, skipping ping"
        fi
    
    - name: Upload failure artifacts
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: failure-artifacts-${{ github.run_number }}
        path: |
          podcast_monitor.db
          youtube_transcripts.db
          daily_digests/
          transcripts/
          state/yt_last_push.json
          **/*.log
        retention-days: 14
        if-no-files-found: warn
    
    - name: Create job summary
      if: always()
      run: |
        echo "## ðŸ“Š Daily Digest Pipeline Summary" >> $GITHUB_STEP_SUMMARY
        echo "- **Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Mode:** ${{ github.event.inputs.mode || 'scheduled' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Run ID:** \`${{ github.run_id }}\`" >> $GITHUB_STEP_SUMMARY
        echo "- **Commit:** \`${{ github.sha }}\`" >> $GITHUB_STEP_SUMMARY
        echo "- **Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        
        # Add digest counts if available
        if [ -d "daily_digests" ]; then
          DIGEST_COUNT=$(find daily_digests -name "*_digest_*.md" -type f | wc -l | tr -d ' ')
          AUDIO_COUNT=$(find daily_digests -name "*_digest_*.mp3" -type f | wc -l | tr -d ' ')
          echo "- **Digest Files Created:** $DIGEST_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- **Audio Files Created:** $AUDIO_COUNT" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Add database status
        if [ -f "podcast_monitor.db" ]; then
          RSS_COUNT=$(sqlite3 podcast_monitor.db "SELECT COUNT(*) FROM episodes;" 2>/dev/null || echo "unknown")
          echo "- **RSS Episodes in DB:** $RSS_COUNT" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -f "youtube_transcripts.db" ]; then
          YT_COUNT=$(sqlite3 youtube_transcripts.db "SELECT COUNT(*) FROM episodes;" 2>/dev/null || echo "unknown")
          echo "- **YouTube Episodes in DB:** $YT_COUNT" >> $GITHUB_STEP_SUMMARY
        fi