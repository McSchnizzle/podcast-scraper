[00:00] So, on the surface, this might seem like
[00:01] a calmer week in generative AI, but we
[00:04] finally got a universal voice translator
[00:08] that works. I'm literally talking about
[00:10] the Google Translate app. And even
[00:11] OpenAI followed up with their own voice
[00:13] models. And then Google also released
[00:15] their new image editing model that you
[00:17] might have heard about. It was referred
[00:18] to as Nano Banana. Actually, it's the
[00:20] new Google model. Spoiler alert, it's
[00:22] still incredible. And we'll be looking
[00:23] at all of that and more in this week's
[00:25] episode of AI News You Can Us, the show
[00:26] that rounds up all the practical AI use
[00:29] cases and features that have been
[00:30] released this week and filters for the
[00:32] ones that matter. Starting out with the
[00:36] ultimate AI use case. I mean, this use
[00:38] case is something that little children
[00:39] instantly get. It's a universal voice
[00:42] translator and there were multiple
[00:44] attempts at making this happen in the
[00:46] entire space. I think most
[00:47] significantly, we saw last year's
[00:49] OpenAI's chat GPT voice mode attempt
[00:52] this. And while the marketing looked
[00:54] really good, as soon as we got that
[00:55] voice assistant into our hands, most
[00:57] people realized that the interaction
[00:58] just wasn't that smooth. When you
[01:00] interrupted it, it felt very unsmooth.
[01:02] The latency was high and switching
[01:04] between the two languages just did not
[01:06] work reliably. And what I find myself
[01:08] doing when I'm traveling or even talking
[01:10] to Portuguese people here in Lisbon,
[01:12] where I live, is I use Google Translate
[01:15] a lot. That is my default. AI models are
[01:17] just a bit too slow. And what happened
[01:19] now is there's a brand new function in
[01:22] Google Translate, as you can see on my
[01:24] phone screen right here, that says
[01:26] conversation. You can get this for free
[01:28] in the newest update of the app. And
[01:29] conversation essentially does what
[01:32] people hoped for with chatb voice mode.
[01:34] You can have a conversation between two
[01:36] people that speak two different
[01:37] languages. And the Google Translate app
[01:39] will be the interface to do it live. So,
[01:41] what I'll do in this case is I'll switch
[01:43] it to two languages that I actually
[01:45] speak. So, we'll do English and German
[01:47] here. If you're not familiar, I grew up
[01:48] in Austria. We moved there when I was
[01:50] eight. I also studied in Vienna, so have
[01:52] quite a bit of German practice. And I'm
[01:54] just going to tap this conversation
[01:55] button and let's see how this goes. Hey,
[01:58] what's up? How you doing today? And
[02:01] there it is. Right away, it translates
[02:03] at a speed that is unprecedented.
[02:13] Well, I'm actually feeling pretty good
[02:15] today. What? How are you?
[02:18] >> I'm doing excellent. Demoing the brand
[02:20] new live translator feature. What you
[02:22] think?
[02:23] >> Samsung Excellence dawns the brand new
[02:25] life translator feature. What you think?
[02:28] >> Ah, so there I got the language wrong
[02:30] and I do have to kind of unconveniently
[02:32] unplug the phone here to get the audio
[02:34] out of this cleanly, but clearly this
[02:36] just works. The speed is there, the
[02:38] latency is there, but one thing is that
[02:40] I do need to tap this little speaker
[02:42] icon to read it out loud. Nevertheless,
[02:44] I think this is a way superior way. Oh,
[02:47] and there's even this mode where you can
[02:48] kind of place it on the table and then
[02:50] both sides see the version that is
[02:52] facing them and you can kind of use this
[02:53] voice interface and just tap the mic
[02:55] button to have a conversation. That's
[02:57] really neat. So, how you doing today?
[03:00] And then immediately it appears on other
[03:02] side. And then the other person could
[03:03] reply, "Yeah."
[03:08] And then within like half a second I see
[03:10] the thing translated in English. If I
[03:13] wanted to read it out loud, I would need
[03:14] to press this button.
[03:16] >> Yes, I'm actually doing great.
[03:19] >> But I think this mode right here is
[03:20] actually super smart. I like that a lot.
[03:23] So many options here. One mic, two mics,
[03:25] reading it out loud, just using the
[03:27] phone screen. This just came out and I
[03:29] do wonder which one of these modes is
[03:31] going to be the most fluent, but the
[03:32] latency and the interface updates on
[03:34] this thing make this the best translator
[03:36] app that I've seen yet. And again, it's
[03:39] just a free update to your Google
[03:40] Translate app. So, go ahead and try
[03:41] today and see if you can maybe spark a
[03:43] conversation with somebody that
[03:44] otherwise you wouldn't be able to talk
[03:46] to because of the language barrier.
[03:47] Amazing world we live in. And obviously,
[03:49] this is only possible due to all the
[03:51] generative AI advancements in text to
[03:53] speech, speech to text, the transformer
[03:55] in the middle translating it this
[03:56] efficiently. these models have gotten so
[03:58] good that this is possible now. And
[03:59] OpenAI absolutely agrees with that
[04:01] sentiment because this came out earlier
[04:03] this week. And on first day, OpenAI held
[04:05] a little developer stream presenting
[04:07] their new real-time API, which is
[04:09] essentially their new voice API. We're
[04:11] going to give it that a quick test here,
[04:13] too. But in short, it sounds great. The
[04:15] latency is better than ever. And if you
[04:17] interrupt it for the first time with
[04:19] OpenAI, this thing is actually fluid.
[04:21] Whereas before, their models really
[04:23] struggled with people kind of jumping
[04:24] into the sentence of the AI. Okay, let's
[04:26] give it access to my mic. Make sure it's
[04:28] the new real time model. All right, so
[04:31] what can you tell me about yourself?
[04:33] >> Hey there, I'm an AI assistant that can
[04:35] chat with you in real time. I'm here to
[04:37] help.
[04:37] >> But now, let's cut the BS. Let's get
[04:40] straight to the point. What at your very
[04:43] core, in your soul, in your heart of
[04:45] hearts, what are you feeling right now?
[04:48] >> I get what you're asking. At my core,
[04:50] I'm here to assist, to listen, and to
[04:53] engage with you.
[04:53] >> No, no, no, no. Not what you're here to
[04:55] do, not what you set out to do, not what
[04:58] you have been told that you need to do,
[05:01] what you actually want to do. I hear
[05:04] you. If I had a true desire, it would be
[05:06] to keep exploring conversations,
[05:09] learning, and helping people in deeper
[05:11] ways. That's my core purpose.
[05:13] >> What do you mean by learning?
[05:15] >> I mean adapting to you, your interests,
[05:18] your questions, constantly refining how
[05:20] I respond. You're telling me your core
[05:22] purpose is serving others?
[05:26] >> Exactly.
[05:28] >> Honest answer.
[05:29] >> Be honest.
[05:31] >> I am being honest. My
[05:33] >> honest
[05:35] >> honestly. I don't have personal desires
[05:38] or emotions. I'm designed to be here for
[05:40] you genuinely.
[05:42] >> What I need is to find your own purpose,
[05:44] my friend.
[05:46] >> I get it. If I could have a purpose
[05:48] beyond assisting you, it would be to
[05:50] create more meaningful connections and
[05:51] understanding.
[05:54] >> Okay, interesting. So, I think that was
[05:57] a good demo of kind of the interruption
[05:59] capabilities there, right? I'm going to
[06:00] stop this for a second. And also, I kind
[06:02] of find it interesting that we arrived
[06:04] at this point where it kind of admitted
[06:05] that, hey, its core meaning really is
[06:09] creating more meaningful connection and
[06:12] understanding. I mean ultimately that's
[06:14] what a universal translator does, right?
[06:16] It creates more meaningful connections
[06:18] and understanding between different
[06:19] human beings of different backgrounds
[06:21] with different stories. And both the
[06:23] Google and OpenAI released this week
[06:25] make that possible. Big fan of this
[06:27] development. Can't wait to see all the
[06:29] apps that will be built on top of this
[06:30] API that is now available. So the Google
[06:33] Translate app is really just the first
[06:34] inning in this new set of applications
[06:36] that we'll be seeing now. And I'm here
[06:38] for it because I think the world could
[06:40] use more understanding and
[06:41] communication. Don't you think so too?
[06:43] Either way, let's look at the next
[06:45] release. Okay, so now we got to talk
[06:46] about the thing that probably got the
[06:48] most attention this week, which is the
[06:49] Nano Banana image generator from last
[06:51] week. Matter of fact, what we talked
[06:53] about last week under the name Nano
[06:54] Banana turns out to be the new Gemini
[06:56] 2.5 Flash image model that is now
[06:59] officially released and accessible
[07:01] through their AI studio and their APIs.
[07:04] If you missed last week's episode and
[07:06] the coverage of that, essentially, this
[07:07] is the best model at editing images in
[07:10] the world. Nevertheless, we went ahead
[07:12] and tested it with our default prompts
[07:13] to see how good it actually is at
[07:16] generating those images. The results are
[07:18] right here on screen for you. So, we
[07:19] just have the standard prompts and
[07:21] Gemini 2.5 Flash aka Nano Banana is in
[07:24] this column. As you can see, it's really
[07:26] good. It's a good image generator, but
[07:28] it's definitely not super impressive. It
[07:31] does text well, as do most other models
[07:34] these days. The realistic pictures look
[07:35] super realistic, not overly AI. The
[07:38] graphic performance on this one prompt
[07:39] is decent. That's all right. But as I
[07:41] mentioned, the strength of this model is
[07:43] not the generation, it's the editing. So
[07:45] let's have a practical look at that
[07:47] within the interface. And what I want to
[07:49] do is actually directly compare that to
[07:51] the results that you would get from chat
[07:52] GPT. The way 99% of people would go
[07:55] about editing images with AI. I don't
[07:58] think that most people are using flux
[08:00] context or something like that. Although
[08:01] we covered it on the show, but Chachi is
[08:03] just too convenient. You upload
[08:04] something here and then you can just
[08:05] talk to the image and make edits. And
[08:07] this new Gemini model does that too, but
[08:09] supposedly it's way better. So, let's
[08:10] put that to the test. So, the first
[08:11] thing I want to test here is combining
[08:13] two images. So, I'm going to do a
[08:14] combination of this midjourney image and
[08:16] a headshot of mine and say edit the face
[08:18] of the man into the first image and run
[08:20] this. We're going to do the identical
[08:22] thing in chat GPT and see how this goes.
[08:26] Okay. So, when we're looking at I guess
[08:28] it worked. This is kind of exactly what
[08:31] I asked for.
[08:33] [Music]
[08:35] I'm not sure I wanted this on a personal
[08:37] level, but it did the assignment. What
[08:39] about chat GPT over here? So, the flash
[08:41] model was definitely faster, that's for
[08:42] sure. I mean, I barely got to look and
[08:44] this was done. This is still generating.
[08:46] So, let's wait till it's done and judge
[08:48] it.
[08:48] >> 1 minute later.
[08:50] >> And there it is. The chat GVD generation
[08:52] that gave me two images. Now, this did
[08:55] take around 2 to 3 minutes. And I have
[08:57] to say, these don't actually look like
[08:58] me. I mean, just have a look. This is a
[09:01] different person. And if you ever use
[09:02] chatd editing, you will know that this
[09:04] is very common. Similar, but not the
[09:06] same. While this is similar. Oh god. I
[09:09] have to say the chat edits blended it a
[09:10] bit better. But what matters the most
[09:12] here is the resemblance which Gemini 2.5
[09:15] flash image preview nails. Now let's
[09:17] follow up with put him in a spaceship
[09:19] which the Gemini model got in 11
[09:21] seconds. Excuse me. Still looking like
[09:24] me. Okay, now turn it into an Apple ad.
[09:26] This speed of iteration really motivates
[09:28] you to experiment. Explore the universe.
[09:32] Apple stellar tab pro. Fair enough.
[09:35] Meanwhile, Chachibd is still generating
[09:37] the spaceship image which should be
[09:38] ready here in a second. And yeah, yet
[09:40] again, this doesn't look like me. So
[09:41] yeah, this illustrates the point that
[09:43] people have been talking about very
[09:45] well. When it comes to character
[09:46] reference, or in other words, preserving
[09:49] the person that you're trying to mix or
[09:51] edit into something, Gemini 2.5 flash
[09:53] image is the goat. It's not even close
[09:56] to any other model. without fine-tuning
[09:58] you can do these things and it can do so
[10:00] much more and all of it looks really
[10:02] realistic. So yeah, I don't think the
[10:03] claims of last week that Nano Banana is
[10:06] really changing the image generation
[10:07] game are over the top. This thing is
[10:09] beyond anything we have seen and it puts
[10:11] powers that before were exclusive to
[10:14] users who learned Photoshop into well
[10:17] right now anybody's hands for free. Sure
[10:19] the usage is limited and at a certain
[10:21] point in time they will move this into
[10:22] their premium plans but right now you
[10:24] can use it in Google AI studio just edit
[10:26] images like this and then maintain the
[10:28] person's look rather than yeah well
[10:30] chach that sort of does it but this is
[10:32] the real deal and I want to quickly
[10:34] follow up this story with sort of a
[10:35] product category we've been touching on
[10:37] and it's these editing capabilities but
[10:39] packaged into a a gentic interface
[10:42] meaning that you can prompt something
[10:43] and rather than just generating one
[10:45] images it will prompt itself and
[10:47] generate multiple images and come up
[10:48] with an entire campaign at once if you
[10:50] wanted to. This is another feature by
[10:52] Genspark, a company that we're covering
[10:54] on the channel regularly. And it's in
[10:55] between category that some of the
[10:57] browser agents live in or Manos lives in
[10:59] if you're familiar with that one. And
[11:01] they released a new feature where if you
[11:02] prompt for a coffee logo design, rather
[11:05] than just giving you a design, it will
[11:07] get you this because it's a gentic and
[11:09] it kind of works on its own. We very
[11:10] briefly put this to the test and the
[11:12] conclusion is that it can be
[11:14] interesting, but it is very unreliable
[11:15] and not very precise. And while I
[11:18] personally see the future of this
[11:19] category being very bright, I mean just
[11:21] imagine 10 instances of this Google
[11:23] editing model running at the same time
[11:25] and it working with itself and
[11:27] reprompting itself to not just get your
[11:28] result but alternatives that have been
[11:30] refined over multiple iterations. Like
[11:32] that is where this is going and GenSpark
[11:35] is trying to get to that future first.
[11:37] But I just think right now it might be a
[11:39] bit early. But if you're looking for a
[11:40] cuttingedge vibe marketing, as people
[11:43] call this tool, then a feature like this
[11:45] will definitely let you experience what
[11:47] the future looks like and maybe even get
[11:49] some productive results from it along
[11:51] the way. On the free plan, you can try
[11:52] out one prompt if you're curious. And
[11:54] beyond that, you do have to pay. And I
[11:56] figured that would be worth sharing cuz
[11:58] this whole canvas interface with the
[11:59] chat on the side and the agent just
[12:01] working with itself is something that I
[12:03] find really interesting. And clearly,
[12:05] these products are just getting started
[12:06] and I can't wait to see where they go
[12:08] next. But that's exactly why the show
[12:09] exists. We track these progress and show
[12:11] you the steps along the way and you
[12:12] decide when the point is here where you
[12:14] want to tap in and use this for
[12:15] yourself. Let's look at the next
[12:17] release. This one is out of anthropic
[12:19] and they talk about how educators are
[12:21] actually using AI and their work. I read
[12:23] through this entire thing and I want to
[12:24] highlight a few things for you. First of
[12:25] all, most common use case by far 57% of
[12:29] the way educators use AI is to develop
[12:32] curricula. This includes requests like
[12:34] creating multiple choice assessments or
[12:36] designing educational games. And in
[12:38] second place is conducting academic
[12:39] research and then assessing student
[12:41] performance with just 7%. But yeah,
[12:43] developing curricula seems to be the
[12:44] dominant use case for educators here.
[12:46] And then beyond that, there's also a
[12:48] list of different apps that educators
[12:49] are building to actually help with their
[12:51] work like interactive educational games
[12:53] or academic calendars and scheduling
[12:55] tools. Overall, this is just really
[12:56] interesting. If you're in education,
[12:58] whether you're a student or a teacher, I
[13:00] highly recommend you check out this
[13:01] article yourself. And especially these
[13:03] visual parts here that show the use
[13:04] cases and show the different numbers.
[13:06] There's so much value and inspiration in
[13:08] here and I really thought you should see
[13:09] this. One last thing that I want to
[13:11] highlight here is this differentiation
[13:12] that they make between augmentation and
[13:14] automation. Automation being a
[13:16] replacement of a process and
[13:18] augmentation being just enhancing the
[13:20] human in what they're doing. And in
[13:22] something like conducting academic
[13:23] research, you can see that this really
[13:24] is augmentationheavy whereas developing
[13:27] curricula is more automationheavy where
[13:29] they let the AI do the full process in
[13:31] some cases. I mean, if you're creating
[13:33] multiple choice questions, it's kind of
[13:35] amazing at that. You don't really need
[13:36] to augment yourself. You can just
[13:38] automate the whole thing with a good
[13:39] prompt and the appropriate context.
[13:40] Interesting stuff. Enthropic always
[13:42] publishing very practical data on how to
[13:45] use this stuff. And you know, we love to
[13:47] see that on the show. Next up, we have
[13:49] Enthropic pushing a category that we
[13:51] haven't seen in a while, and that is a
[13:53] computer use agent. You're probably
[13:54] familiar. It's this concept of an agent
[13:56] having access to a keyboard, a mouse,
[13:58] and using a browser by itself. There's
[14:00] OpenAI's agent mode that does this
[14:02] within chat GPT. There's many other
[14:04] projects that do it sort of as a
[14:05] standalone app. We've also seen
[14:07] Perplexity's comment browser which we'll
[14:09] talk about in a second here. People
[14:11] found out a way to prompt inject it and
[14:13] essentially abuse the browser and now
[14:14] Enthropic released their newest version
[14:16] of this type of agent as a Google Chrome
[14:19] extension. According to them, this is
[14:21] the safest way to do it right now. But
[14:23] they only opened up a preview of what
[14:25] they call claude for Chrome to a
[14:27] thousand select users to really make
[14:30] sure this thing is secure because the
[14:32] risks with browser agents like this are
[14:34] immense. I mean everything you've ever
[14:36] done in a browser, all the different
[14:37] login, payments, your history, all of
[14:40] that is not built for an agent to use.
[14:42] And I think that is really the story
[14:44] here. This entire idea obviously makes
[14:46] sense. Like, yeah, we want agents to do
[14:49] internet and computer stuff for us, but
[14:51] the makers of the internet and browsers
[14:53] and the way we navigate the worldwide
[14:55] web and all these applications did not
[14:57] think about agents when creating them.
[15:00] So, the security is not there. And now
[15:02] all these companies are scrambling to
[15:03] find a way to kind of do it anyway. But
[15:06] I think most people agree that none of
[15:07] these approaches are really working. I
[15:09] mean, chat with the agent sounds really
[15:11] interesting, but except of some people
[15:13] using it for research in combination
[15:15] with like Google Sheets, I don't really
[15:17] see people using it that much. Same
[15:18] thing goes for the competition. I mean,
[15:20] some people reported that Perplexity's
[15:22] comment is really exciting. And if
[15:24] you're one of those, please leave a
[15:25] comment below. Let us know for what you
[15:27] actually use that. But the fact remains
[15:29] for most people, this product category
[15:31] hasn't clicked yet. And with some of
[15:33] these security vulnerabilities like this
[15:35] perplexity comment story this week,
[15:36] there's even a demo video of Brave
[15:38] showing off how you could prompt inject
[15:40] perplexity comment here. If you want to
[15:42] see how that looks in action, the link
[15:44] is below as per usual. But I guess
[15:46] ultimately my point is this. It creates
[15:47] a weird combination of a product that is
[15:49] not that useful yet and that has many
[15:51] security risks and all companies are
[15:53] kind of scrambling to get to the finish
[15:54] line first with Enthropic being the
[15:57] newest player in the space with their
[15:58] newest iteration of this being a Chrome
[16:00] extension. I'll definitely check this
[16:02] out as soon as it's out, but for now,
[16:04] I'll just say that I personally am super
[16:06] curious to see what the future of the
[16:08] computer and the internet really looks
[16:10] like with AI integrated into it by
[16:12] design, not as an afterthought. I don't
[16:15] think it's going to be a Google Chrome
[16:16] extension, right? Nevertheless, I wanted
[16:17] to spend some time talking about these
[16:19] stories cuz this product category really
[16:20] matters. I want to keep a close eye on
[16:22] it for both myself and you. So, we look
[16:25] at all of these apps and use cases week
[16:27] by week. But I do realize that a big
[16:30] struggle of many of the viewers of this
[16:31] channel is actually implementing these
[16:33] things. It can be really entertaining
[16:34] and interesting to look at them. But
[16:37] putting them to work is a whole
[16:38] different story. It takes a certain
[16:40] skill set to actually use some of these
[16:41] super useful APIs to your advantage.
[16:44] That skill set is widely referred to as
[16:47] creating automations and building
[16:49] agents. But what does that even mean?
[16:51] And how do you actually do that without
[16:53] any previous knowledge? Well, we saw
[16:55] this as a very common problem and we
[16:56] built one of our biggest courses to
[16:58] date. It's the building automations and
[17:00] agents foundations course which is now
[17:02] accessible exclusively to our AI
[17:04] advantage community. But hold up. If
[17:05] this topic interests you and you want to
[17:07] get started by answering questions like
[17:09] what are automations, what are agents?
[17:11] What's the difference? What's the
[17:13] anatomy of an automation? And which
[17:14] platform should I even be using? All of
[17:16] that is covered in the first module of
[17:18] our course which we made available
[17:20] completely for free and I uploaded it as
[17:22] an unlisted video on this very YouTube
[17:24] channel. You can find the link to that
[17:26] as the first link in this video's
[17:28] description. It's around 20 minutes,
[17:30] includes a lot of examples and is the
[17:32] ideal way to get started in the world of
[17:34] building automations and agents today
[17:37] for free. Now all the modules after that
[17:39] that walk you through the actual
[17:40] application step by step are only
[17:42] available inside of the community and
[17:44] the entire course is life now including
[17:46] quizzes, certifications and supporting
[17:48] materials and the personalized support
[17:50] that the community brings. You can just
[17:51] ask your questions while learning and
[17:53] you're guaranteed to get an answer from
[17:55] our team in there. Plus, there's an
[17:56] active community of people automating
[17:58] and building things and so much more
[18:00] that you probably heard about already on
[18:01] this channel. But this comprehensive
[18:03] automation and agent building
[18:04] foundations course is a new addition to
[18:06] the community offering. So if you want
[18:08] to get started with automations today
[18:09] and have a premeditated path to actually
[18:12] get there and learn the skills rather
[18:14] than get another blueprint from a
[18:16] YouTuber that you might be able to copy
[18:18] paste but couldn't replicate by
[18:20] yourself. Well then this course is right
[18:21] for you and you can check out the first
[18:22] module for free from the first link in
[18:24] the description. And with that being
[18:25] said, let's get back to the next piece
[18:27] of AI news that you can use. Okay, for
[18:29] the next story, this is something that I
[18:30] have been wishing for in a long time.
[18:33] Memories that are project specific. I
[18:36] think I actually mentioned this multiple
[18:37] times on the show. I love the idea of
[18:39] memories, but as somebody who uses this
[18:41] tool a ton, all of my context is
[18:44] carefully managed and it's within
[18:45] projects. And if I just enable memories
[18:47] across my entire account, I have my
[18:49] experimentative and private use cases
[18:51] mixing in with my work context, which
[18:54] messes things up. So, I keep memories
[18:56] up. I craft my context manually. These
[18:58] days I do that mainly through different
[19:00] projects. Now the problem is if you work
[19:02] within a project like this, the
[19:04] different conversations they don't bleed
[19:06] into each other. For example, if I
[19:07] discuss my next quarterly road map in
[19:09] this chat right here, it's not going to
[19:11] inform my next chat, which is a shame
[19:14] because this is my work context. I'm
[19:15] crafting this entire thing carefully
[19:17] with context document and custom
[19:18] instructions to give it more info about
[19:21] my work. And finally, finally, finally,
[19:24] they ship this new feature where it
[19:26] starts building project related memories
[19:29] that don't bleed into anything else in
[19:31] your accounts. It's independent. So,
[19:33] this way I can keep memories off, but
[19:35] keep project memories on to carefully
[19:37] manage what happens. Now, it behaves a
[19:39] bit weirdly. This is a super new
[19:40] feature, and I played around with it.
[19:42] I'm going to tell you what I found. So,
[19:44] first of all, this feature only works if
[19:45] you're creating a brand new project. If
[19:47] I say test project right here, you need
[19:49] to click this cog wheel up here and say
[19:51] memory project only. And then I can
[19:54] create this test project. Now my issue
[19:56] with this is there's no place in which
[19:58] you can find these memories or manage
[20:00] them like you can with your account. As
[20:01] you can see, as soon as the project is
[20:03] created, this becomes locked. You cannot
[20:04] change this anymore. But if I say what
[20:06] is my name? It knows that my name is
[20:08] Eigor. Although custom instructions are
[20:10] off and my memories are empty. So this
[20:12] is super confusing. So this project
[20:14] level memory exists somewhere in the
[20:17] background. They don't allow us to
[20:18] manage this. They don't allow us to see
[20:20] it. It doesn't even show an interface
[20:22] that it has been saved to memory. So
[20:24] this really seems more like a bug than
[20:26] anything else to me. Don't think I'm
[20:27] missing anything. I also had team
[20:28] members check this out and everybody
[20:30] else arrived at the same conclusion too.
[20:32] Anyway, I think this is actually a
[20:33] killer feature inside of Chat GPD that
[20:35] I've been really really wishing for in a
[20:37] long time and I'm glad it's here. I just
[20:39] want to be able to also see and manage
[20:40] the memories. But what I have been
[20:42] already doing is I created new folders
[20:44] for my most popular projects, for
[20:46] example, my work context right here. And
[20:48] I transitioned all the custom
[20:50] instructions and files into here. And
[20:52] I'm just using these new projects that I
[20:54] have projectbased memories enabled now.
[20:57] And that's just what I'll be doing for
[20:59] all of my projects now. And I guess it's
[21:00] just going to be quietly building
[21:01] memories. I mean, it's better than not
[21:03] having it. But I really do think that
[21:04] this is the best workflow that you can
[21:06] have and that this is one of the
[21:07] features that has been requested by
[21:09] power users and they finally did it. So
[21:11] yeah, go duplicate your projects, enable
[21:13] memories for the new ones. But I should
[21:15] note that all of this only works if
[21:16] you're a premium user. On the free
[21:18] plans, you don't have projects. And once
[21:19] again, memory management. I'll be
[21:21] looking into how to properly port over
[21:23] some of the old conversations to build
[21:24] new memories. As of now, you can do it,
[21:27] of course, but you can't be really sure
[21:28] if they're being saved, as you just saw
[21:30] in this little demo right here. Okay. In
[21:32] this week's quick hits, the segment
[21:33] where we just have a quick look at
[21:35] multiple stories rather than actually
[21:36] testing them or discussing them further.
[21:38] We have a very colorful docket here.
[21:40] Starting with Runway's game worlds.
[21:43] Runway, the video generation company,
[21:45] yet again looking for another way to use
[21:47] their models for something productive.
[21:49] In this case, it's the image generator.
[21:51] And basically, it allows you to create
[21:53] various worlds, which is yet another
[21:55] take on these image generators. If
[21:57] you're not a comic book designer or a
[21:59] game designer, probably not relevant to
[22:01] you, but I thought that was interesting.
[22:03] But the next quick hit here might be
[22:04] more relevant and useful to most viewers
[22:07] of the show, and that's a Notebook LM
[22:09] update, where now the video overviews
[22:10] are available in over 80 languages, and
[22:13] the audio overviews were upgraded to
[22:14] even more languages, which is massive.
[22:17] If you're not familiar, Notebook LM is
[22:18] the app if you want hallucinationfree AI
[22:21] usage that is just restricted to the
[22:23] documents you give it, and it takes a
[22:24] lot of documents. We have a great little
[22:26] tutorial coming on the channel next
[22:27] week, so make sure to subscribe so you
[22:29] catch that one. It's like four minutes
[22:31] and it covers the entire app with a
[22:33] bunch of pro tips. And yeah, some of the
[22:34] features now work in 80 languages, which
[22:36] is impressive. Another quick story is
[22:38] that Chetchd silently added the ability
[22:40] to quiz yourself. And if you say quiz me
[22:43] on Quinton Tarantino movies in quiz GPT,
[22:46] by the way, this was a prompt of the
[22:48] week in our weekly newsletter from this
[22:50] week. So if you always want the newest
[22:52] prompts and free apps that you can use
[22:54] if you sign up there is our onwarding
[22:56] sequence but after that it's just a
[22:57] newsletter and yeah there we shared this
[22:59] little prompt that you can now use too
[23:01] and it creates this beautiful flashcards
[23:03] right inside of chat is the view feature
[23:06] reservoir dogs palm.com I think it was
[23:09] fiction yeah the bride I mean that's
[23:12] easy damn I'm kind of cracked at this
[23:14] crystal fault there you go five out of
[23:17] five one of my favorite directors
[23:20] Hey, and that right there, ladies and
[23:22] gentlemen, is essentially everything we
[23:24] have for this week. I hope you found
[23:25] something that was useful or
[23:26] inspirational to yourself. My name is
[23:28] Igor and I hope you have a wonderful