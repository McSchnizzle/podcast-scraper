[00:00] This week in generative AI, we have
[00:01] several apps that can do Photoshop level
[00:04] edits just by speaking to them and some
[00:06] interesting standardization coming in
[00:07] when it comes to how you provide agents
[00:09] with context. That and a few more
[00:11] stories in a lightweight edition of AI
[00:12] news you can use. The show that looks at
[00:14] all the releases in generative AI from
[00:16] the last week and highlights the ones
[00:17] that you can use or that matter.
[00:19] Starting off the story that everybody
[00:20] has been talking about over the past few
[00:22] days. Nano Banana, that's the name of
[00:24] this tool that has been going around on
[00:27] LM Arena. And if you're not familiar,
[00:29] this is a platform that ranks various
[00:31] models on their capabilities. In this
[00:33] case, Nano Banana is a image editing AI
[00:36] model. So you give it an image and words
[00:38] and it edits them. You might have
[00:39] experienced this concept in Chat GPT.
[00:41] When you upload a picture and ask for
[00:42] some edits, it can do that too. But as
[00:45] you might know in chat GPT with the
[00:47] image generation tool, if you try to
[00:49] edit, it will change everything about
[00:50] the picture, including the look of the
[00:52] person that you're trying to edit,
[00:53] making it not even an option for most
[00:55] use cases. Nano Banana is different, but
[00:58] it's not like a fully public available
[01:00] thing. People are rumoring that Google
[01:02] is testing this tool against other
[01:03] models on LM Arena. And right now, the
[01:06] only way to really try to yourself is to
[01:08] have a little patience and to cycle
[01:10] through various tools while comparing
[01:12] two of them in their interface. And then
[01:13] eventually, you're going to find one
[01:15] with the name Nana Banana. We did this
[01:17] for you and tested the image editing
[01:18] capabilities with a prompt that you can
[01:21] now see compared to some of the
[01:22] competition. Now, here's the thing about
[01:25] the model. It's very good at editing
[01:27] stuff locally and adhering to your
[01:29] prompt. Meaning, if you tell it you only
[01:31] want to edit the jacket and you want to
[01:32] make it a certain material, it will
[01:34] actually do it. Whereas most other
[01:36] models are kind of hit and miss, as you
[01:37] can see in these examples, it has its
[01:39] weaknesses with the text, but I was
[01:41] actually kind of impressed by how good
[01:43] it is at, for example, replacing a piece
[01:45] of clothing like we did in this demo
[01:47] here. And this is giving Photoshop a
[01:49] real run for its money because this
[01:50] classic tool to alter images has a
[01:53] learning curve. Not everybody can just
[01:54] use that intuitively, whereas everybody
[01:56] can type what they need into a little
[01:58] text box and get results. And changing
[02:01] jackets is just one use case if you
[02:03] really have the power to accurately edit
[02:05] images with just words. So you could do
[02:08] image restoration like turning these
[02:09] historical photos into something
[02:11] current. And look at the accuracy
[02:13] between those two. You can change the
[02:14] lighting in the scene or take one base
[02:16] image and recreate that in many other
[02:18] copies. I mean, look at the character
[02:20] reference here between the various
[02:22] shots. And over here they look
[02:24] surprisingly similar. Whereas with all
[02:26] other models, usually they just look
[02:27] like a different person. Chachi being
[02:29] actually one of the worst ones when it
[02:30] comes to image reference. So this is
[02:32] really pushing what's possible with
[02:34] these image models. And we had even a
[02:36] second model that I kind of want to
[02:37] follow the story up with released out of
[02:39] Quen this week. Quen image edit. And
[02:42] they lead with this example of one
[02:44] character into all of these variations.
[02:46] And again, the character reference on
[02:48] this is becoming so strong. We've never
[02:50] really seen it before at this level.
[02:52] Look, this is the input image and then
[02:53] you can create all of these variations.
[02:55] They have a lot more examples on their
[02:57] blog post. And if you actually want to
[02:58] try one of these tools, this is the one
[03:00] that's easy to access. If you go to
[03:01] chats.quen.ai, you can go in here and
[03:03] use the image editing tool right in
[03:05] here. And me and a team have actually
[03:07] been playing with these tools.
[03:08] Particularly, this Quinn model is
[03:10] actually really good at text. And you
[03:11] can do things like giving it a thumbnail
[03:14] like this one of me riding a camel
[03:16] looking for the best AI search engine.
[03:18] And then if I tell it to change the text
[03:19] to best burrito in town, just don't
[03:21] forget to enable image edit right there.
[03:23] Let's have a look. And that is not
[03:25] perfect. Let's give it one more try.
[03:27] Meanwhile, I'll show you an example that
[03:29] came up during our testing here. It just
[03:31] nailed it right. The gradient stayed the
[03:33] same. It got the font exactly right.
[03:35] Let's see if we can replicate that here
[03:36] in our live testing. No, it's really
[03:38] having troubles with the word burrito.
[03:40] One final try maybe. No, actually did
[03:42] not work flawlessly. So, it looks like
[03:44] it might work for a word or two, but
[03:45] four is too much. And Nano Banana isn't
[03:48] even good at text. It's just good at
[03:49] everything else. So, let's try one more.
[03:51] Replace the map with a smartphone. Okay,
[03:53] hands are kind of good, but not great
[03:55] either. So, yeah, clearly some of these
[03:56] Nano Banana examples are the most
[03:58] impressive thing we've seen out there,
[03:59] but it's kind of hard to access for now.
[04:01] But interesting as an example of how
[04:03] quick even these image models are
[04:04] progressing these days. One thing that
[04:06] all top AI models like GP5, Gemini 2.5
[04:09] Pro, and Opus 4.1 have in common is that
[04:12] they're incredible at writing code. But
[04:14] just producing good code isn't what
[04:17] truly makes a developer or a dev team
[04:19] great. You need code that works that you
[04:21] can actively improve on. And that's
[04:23] often more important than simply writing
[04:24] it. And when you're testing, the problem
[04:26] is often that if you run the code, you
[04:28] don't exactly see what happened when it
[04:29] ran. And it doesn't matter how smart the
[04:31] model is if it has to guess what
[04:32] happened. It's missing crucial context.
[04:34] And that's why today I want to show you
[04:36] a tool that gives you that missing
[04:37] context. And that's Jam, the sponsor of
[04:39] today's video. So here's how Jam works.
[04:41] If you're in the process of testing a
[04:42] new feature and something doesn't go as
[04:44] expected, instead of having to do the
[04:46] classic AI debugging thing where you
[04:48] take screenshots, copy console output,
[04:50] or sometimes you're even trying to
[04:51] reproduce the buck yourself, instead of
[04:53] all that, you can simply use JAM's
[04:55] instant replay. And then here's what
[04:56] happens. This is really interesting. Jam
[04:58] takes the last few seconds of what
[05:00] happened on your screen, and it fills
[05:02] out the entire ticket for you, including
[05:04] technical details and a step-by-step
[05:06] reproduction and even a suggested fix.
[05:08] It also automatically adds all network
[05:11] requests and console outputs. All the
[05:13] things that you would usually be
[05:14] manually pulling together to give the AI
[05:17] model a chance to debug efficiently. And
[05:19] it packages all that in one sharable
[05:20] link. They even packaged all of this in
[05:22] an MCP so you can easily call this from
[05:24] within cursor or clot code, giving all
[05:26] these apps the ability to essentially
[05:28] look at everything that matters from the
[05:30] screen recording to the logs and really
[05:32] understand what went wrong to give the
[05:34] AI model a chance to fix it. Need that
[05:36] context anywhere else? one with one
[05:37] click you can get it to Jira Linear
[05:39] Asana notion or GitHub you name it.
[05:42] Honestly I can't stress how much time
[05:43] this actually saves even if you're not
[05:45] doing this work yourself if you're
[05:46] working with developer rather than just
[05:48] saying hey I tried the form and it
[05:50] doesn't work and then them following up
[05:51] with multiple questions it's so much
[05:53] easier if somebody's testing the app to
[05:55] actually have this at your disposal so
[05:57] you can hand it off to a developer or
[05:58] hand it off to an AI to really paint the
[06:01] full picture. I've been sitting here and
[06:02] copying console logs and taking
[06:04] screenshots since forever now and this
[06:07] is just a better way to do it. And
[06:08] recently I worked with a developer and
[06:10] we went through this entire process many
[06:11] times and it was such a tedious process.
[06:13] As I learned about Jam I was genuinely
[06:15] upset that I didn't have it for that
[06:17] project. So if you're a developer
[06:18] project manager or QA save time and
[06:21] headaches in your next development
[06:22] project, go check out JAM today. You can
[06:24] get started on a free plan by clicking
[06:26] the link at the top of this video's
[06:28] description. Thanks again to JAM for
[06:29] sponsoring this video. And now on to the
[06:31] next piece of Aan news that you can use.
[06:33] Okay, so this one as a passionate video
[06:35] creator is incredible. This is something
[06:37] that back in the day when I was manually
[06:40] editing videos. If you're not familiar,
[06:42] in my early 20s, I started out by doing
[06:44] cold outreach to various nightclubs in
[06:46] the city that I lived in, Vienna. And
[06:47] then from around 21 to 23, I was
[06:50] creating two free event videos a week.
[06:52] Then I moved to corporate events,
[06:53] weddings, course production, and so on.
[06:56] But really, music was an essential part
[06:58] all along the way. And the only way to
[07:00] get music for my videos back then was
[07:02] licensing it from various sites. And now
[07:05] you might know that with AI you can
[07:06] generate it. But also there's all these
[07:08] new tools like this one that we're
[07:10] seeing now. And to be clear, this is not
[07:12] a first. But this 11 Labs music model
[07:15] has been just so good that I can't wait
[07:16] to try this out. It's a videoto music
[07:18] tool and it does exactly that. And we're
[07:20] going to try it now. We're going to
[07:21] upload a video and see what kind of
[07:23] music it generates for it. So I just
[07:25] have some random AI videos laying around
[07:26] here. Let's see what it does with them.
[07:28] This is one smooth animation that I kind
[07:30] of like. Going to upload this. And
[07:31] without even logging in, this should
[07:33] give us a song for it.
[07:35] [Music]
[07:39] Oo. Yeah, that is so fitting. I wasn't
[07:41] even sure what I would put there myself,
[07:43] but this type of tool, it makes me want
[07:45] to create unique videos again. Okay,
[07:47] let's try one more. There's this little
[07:48] video of, I don't know, animated me
[07:50] waving. Let's see what it does here. I
[07:52] suppose this vibe should be more of a
[07:54] elevator music or or something friendly.
[07:58] Yeah, exactly right. Like a corporate
[08:00] jingle.
[08:02] Okay, I have one more clip right here.
[08:03] Want to see what it does? I mean, it's
[08:04] the pulp fiction scene.
[08:11] That is so fitting. It kind of nailed
[08:13] all three of those. So, you can try that
[08:14] without even logging in. That's kind of
[08:16] nice. And relating to that story, I'm
[08:18] just going to quickly point out that
[08:19] they also made their 11 music product
[08:21] available for the API now. Wasn't the
[08:23] case last week. And they dropped a bunch
[08:25] of guides in their documentation. So, if
[08:26] you wonder how to prompt it, how to
[08:28] really get the most out of this tool,
[08:29] well, they shared various best practices
[08:32] here in their documentation. All right,
[08:33] let's move on. Google made a bunch of
[08:35] announcements. None of that is really
[08:36] available yet, but there's one thing in
[08:38] particular that I really wanted to
[08:40] highlight in this video. They're
[08:41] announced their Pixel 10 with a bunch of
[08:42] AI features and an AI focused chip. It's
[08:45] basically a AI focused phone. And they
[08:47] also had some Google Home announcements.
[08:49] But there was one thing in here that I
[08:51] just really wanted you to see so you can
[08:53] prepare for this future where you can do
[08:54] phone calls and it live translates into
[08:57] any language. I mean this phone is going
[08:59] to be coming as soon as October. And to
[09:01] do low latency live translation, you
[09:03] just need a lot of computing power on
[09:05] the device. And apparently the new chip
[09:07] makes this possible. And you can
[09:08] translate between all of these languages
[09:10] right here. This one in particular I
[09:12] just can't wait to try as soon as that's
[09:13] available. I could use that in Portugal
[09:15] dealing with the super slow bureaucracy
[09:17] where I don't even understand it. many
[09:19] times. Okay, next up we got to talk
[09:21] about a trend that I absolutely love and
[09:23] that's the unification of standards
[09:25] across the entire generative AI space.
[09:28] In this case, I'm talking about this
[09:30] agents.m MD format which is looking to
[09:33] be the universal format for all agent
[09:36] context files. And I think this Twitter
[09:38] post perfectly outlines the utility of
[09:40] this idea. Shatien, the creator of the
[09:43] popular front-end library, shares that
[09:44] this is what agentic projects commonly
[09:46] look like. You have a bunch of different
[09:48] rules. You have instructions, you have a
[09:50] clot.md if you're working with clot
[09:52] code. And this new agent.mnd format is
[09:54] looking to solve that. And the open dev
[09:56] account even acknowledged this week. And
[09:58] that's basically the story. It's just
[10:00] looking to unify all of these context
[10:02] documents that agents use to maintain
[10:04] their memory and give you more
[10:06] consistent results across time. There
[10:08] are essential part of the context that
[10:09] these agents work with. And I hope to
[10:11] see this adopted by all the coding
[10:13] platforms because then it's going to
[10:14] make a lot of stuff interoperable. And
[10:16] now with this file tool, you could
[10:17] easily transfer the different tools and
[10:20] context that you have in certain apps
[10:21] and pick the work up with a different
[10:23] tool, which is just not the case right
[10:24] now. There's quite a bit of lock in as
[10:26] every system functions a bit
[10:27] differently. And I just love this idea
[10:29] of a future where everything is
[10:30] interoperable, open, and the power lays
[10:33] in the user's hand. And this is a step
[10:34] in that direction. And that's why I
[10:36] really wanted to cover this. And now on
[10:37] to this week's quick hits. There's a few
[10:39] stories that I thought were worth your
[10:40] attention. And the first one is chatb
[10:42] typos. Matt Wolf was tweeting about
[10:44] this. This was a hot topic in our
[10:45] community this week and multiple members
[10:47] conferred that they have this same issue
[10:49] of chat chip making typos, something
[10:51] we've never seen before. What this
[10:52] really means is that they're tinkering
[10:54] with the model as it is live. They're
[10:56] not saying what they're changing, but
[10:58] they're switching it up just like they
[11:00] have since the release of GPT5. But this
[11:02] is a really weird one because we've
[11:03] never seen chat make typos before. I
[11:05] mean, what kind of training data are
[11:07] they giving it that typos are in there?
[11:08] If I think about that, that's got to be
[11:11] conversational data. you know, like
[11:12] chats or Discord server rooms. You're
[11:14] not going to get typos in books or video
[11:17] transcripts or even blog posts. Hm. Not
[11:20] sure what to really make of that, but
[11:21] just wanted to tell you about it.
[11:22] Another story that I always love is
[11:24] these gaming benchmarks for LLMs. And
[11:26] there's been a change. GPT5 is the new
[11:28] king because it actually managed to
[11:30] finish Pokémon Red. Okay, so for some
[11:32] fun facts around this, the entire
[11:34] 162hour run cost about $3,500
[11:38] in API credits. So, we did some research
[11:40] and we couldn't find exact times on this
[11:43] exact benchmark to compare to GPT5. But
[11:46] I remember that the Gemini model got a
[11:48] comparable results to 03. And there is
[11:50] this graph that compares it to the 03
[11:51] performance. As you can see, it took
[11:53] 17,000 steps for 03 to complete this
[11:55] whereas GP5 did it in 6,000. Just
[11:57] another way to look at AI progress and
[11:59] how quickly things are coming along. And
[12:01] then also I thought it's super
[12:02] interesting that OpenAI launched this
[12:04] initiative to launch chat GPT in India.
[12:07] They have a brand new subscription tier
[12:08] that costs $4 for 10 times higher
[12:11] message limits than the free version. I
[12:13] want to see the entire world benefit
[12:14] from this technology and these use
[12:15] cases. And I can tell you already from
[12:17] my YouTube analytics, I've seen that
[12:18] India is the country that is consuming
[12:20] English AI content the most outside of
[12:23] the Western world. Sure, there's a lot
[12:24] of people who speak English, but also I
[12:26] think there's a tech literacy and a
[12:28] curiosity. And I hope that the big
[12:29] companies keep serving up plans like
[12:31] this to create more opportunity,
[12:33] especially in the software space. And
[12:34] that right there is pretty much
[12:35] everything we have for this week. It was
[12:36] a bit of a shorter one, but hey, it's
[12:38] summer. This is how I expected things to
[12:40] go. But then we got the week in early
[12:41] August where everything dropped at the
[12:43] same time. At this point, I want to
[12:44] share one thing about the channel, and
[12:46] that is that we're refocusing on new
[12:48] YouTube content. We have several very
[12:50] interesting and higher production value
[12:52] videos in production right now. And I
[12:54] can't wait to share some highv value
[12:56] tutorials and super long- form content
[12:58] that teaches at a level of depth that we
[13:00] haven't covered before on this channel.
[13:02] So, keep your eyes peeled for that. And
[13:03] other than that, we'll keep doing this
[13:05] show every Friday. My name is Igor and I
[13:07] hope you have a wonderful