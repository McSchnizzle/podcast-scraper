[00:00] It's like the most creative satisfaction
[00:02] I've had in my whole life. So, I
[00:04] generated all these clips in a pretty
[00:06] straightforward way. I used GPT40 to
[00:08] help me with the prompts. Said, "Hey,
[00:10] help me capture grunge 1990s Seattle
[00:12] inspired by some of these music videos."
[00:14] And then, as you can see, it gets
[00:15] progressively more like camcorder grimy.
[00:19] So, I generated all this stuff and then
[00:21] I threw it together into a music video.
[00:23] All right, let's watch it.
[00:27] [Music]
[00:28] You get the patented Clairvo raised
[00:31] hands reaction on this one. I cannot
[00:33] believe this is AI generated. It's so
[00:35] high quality. It's so specific and
[00:38] aesthetic in a wardrobe and emotion. You
[00:41] have inspired me after this podcast.
[00:44] What music video am I going to make?
[00:45] It's so much fun.
[00:48] [Music]
[00:50] Welcome back to How I AI. I'm Claire Vo,
[00:53] product leader and AI obsessive here on
[00:55] a mission to help you build better with
[00:57] these new tools. Today we have a fun and
[01:00] inspiring episode with Anish Atraa,
[01:03] general partner at Andre Horowitz and AI
[01:06] consumer investor. But we're not going
[01:08] to talk about portfolio companies or the
[01:10] future of AI. No, we're going to use AI
[01:13] to build music videos, analyze our
[01:16] bookshelf, and help us plan our personal
[01:19] finances. Let's get to it. To celebrate
[01:22] 25,000
[01:23] YouTube followers on How I AI, we're
[01:26] doing a giveaway. You can win a free
[01:29] year to my favorite AI products,
[01:31] including VZero, Replet, Lovable, Bolt,
[01:35] Cursor, and of course, ChatPD by leaving
[01:38] a rating and review on your favorite
[01:40] podcast app and subscribing to YouTube.
[01:43] To enter, simply go to howi ai
[01:47] pod.com/giveaway,
[01:50] read the rules, and leave us a review
[01:52] and subscribe. Enter by the end of
[01:54] August, and we will announce our winners
[01:57] in September. Thanks for listening. This
[02:00] episode is brought to you by Notion.
[02:03] Notion is now your do everything AI tool
[02:06] for work. With new AI meeting notes,
[02:09] enterprise search, and research mode,
[02:11] everyone on your team gets a notetaker,
[02:14] researcher, doc, drafter, brainstormer.
[02:17] Your new AI team is here right where
[02:19] your team already works. I've been a
[02:22] longtime Notion user and have been using
[02:24] the new notion AI features for the last
[02:26] few weeks. I can't imagine working
[02:28] without them. AI meeting notes are a
[02:31] gamecher. The summaries are accurate and
[02:33] extracting action items is super useful
[02:36] for stand-ups, team meetings, one-on-
[02:38] ones, customer interviews, and yes,
[02:41] podcast prep. Notion's AI meeting notes
[02:43] are now an essential part of my team's
[02:46] workflow. The fastest growing companies
[02:48] like OpenAI, Ramp, Versel, and Cursor
[02:52] all use Notion to get more done. Try all
[02:55] of Notion's new AI features for free by
[02:59] signing up with your work email at
[03:01] notion.com/howi
[03:04] aai. Anish, I am so excited to have you
[03:08] here and let me tell you why. It is
[03:10] because I have spent the majority of
[03:12] this podcast talking about enterprise
[03:15] B2B product management, how to manage
[03:19] your manager or manage yourself as a
[03:20] manager or how to vibe code. That has
[03:23] been the topic of how I AI and today we
[03:26] are just going to have a little bit more
[03:29] fun. So why did you start to come to
[03:33] these AI projects that are a little less
[03:35] like workreated or technical and and
[03:39] actually just a little bit more fun? How
[03:40] did you how did you get here?
[03:42] Great. Well, I'm excited to have some
[03:43] fun today. I mean, I've been passionate
[03:45] about music forever. I think most of us
[03:46] are. I've been DJing and making music
[03:48] for 30 years. But music is very
[03:50] constrained. you know, there's only so
[03:52] many ways you can work with it. And an
[03:54] example of that is if you look at a
[03:56] track that has all the instruments mixed
[03:58] down into a final MP3 or wave file.
[04:00] There's no way to just extract the vocal
[04:02] or just extract the drums. So, you're
[04:04] really limited by a set of choices that
[04:07] were made in the studio. And with AI,
[04:10] you can do all this crazy stuff like
[04:11] disentangle a track into just the vocals
[04:13] and just the instrumentation. So, what
[04:15] really got me excited at first was
[04:17] everything you could do with AI and
[04:19] audio. And then that of course fed into
[04:21] all of the new video models and videogen
[04:23] and lip sync and all the new
[04:25] technologies we're seeing. So it's just
[04:26] it's like the most uh creative
[04:29] satisfaction I've had in maybe my whole
[04:32] life.
[04:32] Yeah, I agree with you. One of the
[04:34] things that I have so much fun with AI
[04:36] on is people are really worried that it
[04:38] takes away the most fun, most human,
[04:41] most creative parts of not just building
[04:44] things, but creating music, creating
[04:45] art, creating writing. And I in fact
[04:47] feel like it just gives me so much more
[04:49] tools, so much more breath, so many more
[04:52] things I can play with and and build.
[04:54] And so it really opens up this like
[04:56] creative artist side of me in a way that
[04:59] has been really hard to access as an
[05:01] adult also with limited time.
[05:02] Yeah. No. And it's it's actually a fun
[05:04] conversation we'll have over a glass of
[05:05] wine sometime. But if you look at music
[05:07] culture, music culture has kind of been
[05:08] defined by remix culture for the last 40
[05:11] years. You know, like the mixtape was
[05:13] the first time that you could take the
[05:14] music and do something, you know, the
[05:15] cassette tape and do something of your
[05:17] own with it. And then that of course
[05:19] evolved into, you know, hip-hop, which
[05:21] also sampled and which also had a lot of
[05:23] suspicion on it. But sampling was the
[05:25] foundation of hip-hop. And I think AI is
[05:27] just the next manifestation of sampling,
[05:29] and it'll be as important for music as
[05:31] hip-hop was.
[05:32] Well, and we'll we'll stop opiniding
[05:33] about AI and the arts. But the other
[05:35] thing that this remix culture makes me
[05:37] think about is kind of the next step
[05:39] that we've seen in the past couple
[05:40] years, which is kind of audio and video
[05:43] remixing. This like Tik Tok memes, these
[05:46] dances, these things where you're taking
[05:48] a snippet of creativity, turning it into
[05:50] your own thing, and then releasing it to
[05:53] the world in a in a new version. So, I
[05:55] definitely think we're seeing this not
[05:56] just the audio side, but also at the
[05:58] video side, which brings us to your use
[06:01] case. So tell me what you built or what
[06:05] you created maybe and I'm excited to
[06:07] walk through how you got it done.
[06:09] Amazing. Amazing. Great. Tiny Desk is
[06:12] the best. So if you haven't gotten into
[06:14] Tiny Desk, most people have seen it.
[06:15] It's just it's so cool. It's so fun and
[06:18] of course you know like creativity loves
[06:20] constraints and the constraints of Tiny
[06:22] Desk are incredible. Um there's a really
[06:25] good one from Clips that just dropped
[06:27] last week and I mean anyway there's an
[06:30] infinite number of them. It's a fun
[06:31] format. It's sort of like the unplugged
[06:33] format of the '9s.
[06:35] So, I I love Tiny Desk and I got to
[06:38] thinking about all the artists I'd want
[06:39] to see on Tiny Desk. And, you know, of
[06:41] course, some of them are no longer able
[06:43] to be on Tiny Desk because they're not
[06:44] alive anymore. Um, so that got me
[06:46] thinking about how I could do a
[06:48] notorious B.I. Christopher Wallace tiny
[06:50] desk and do we have the tools and
[06:52] technologies and of course, can we do it
[06:54] in a way that's, you know, respectful um
[06:56] and not derivative? And I did it and it
[06:59] seemed like it kind of worked. Maybe we
[07:00] can cut to it um so your audience can
[07:02] check it out and the workflow is pretty
[07:04] simple.
[07:04] We'll do a little clip of it, I think,
[07:06] and then we can work through how how it
[07:08] got there.
[07:10] [Music]
[07:14] To all the ladies in the place with
[07:16] style and grace, allow me to lace these
[07:18] lyrical dishes in your bushes who rock
[07:20] grooves and make moves with all the
[07:22] mommy. The back of the club sipping my
[07:24] wet is where you find me. The back of
[07:26] the clubing hoes, my crew's behind me.
[07:28] Mad question asking passing music
[07:31] blasting but I just can't quit because
[07:35] Okay, we love it. It's great. And you
[07:37] made that?
[07:38] I did make it. Yes. And it took
[07:40] surprisingly little time. Yeah. So, let
[07:42] me show you exactly how I made it. So, I
[07:44] started with 40. 40 is the best uh
[07:46] general purpose multimodal model um in
[07:49] in my opinion. I use it for everything.
[07:51] Um and I just asked it to generate an
[07:53] image of and we're going to do Kurt
[07:55] Cobain. That'll be fun today from
[07:56] Nirvana, of course. That's from when I
[07:58] was in high school um playing a tiny
[08:01] desk concert. So, let's see what it
[08:04] comes up with.
[08:04] While this is loading, you know, you
[08:06] mentioned that for 40 is the best kind
[08:08] of multimodal all-purpose model. I
[08:10] generally agree. You know, 40 Image Gen
[08:13] had this super viral moment a couple
[08:16] months ago when they they released it.
[08:18] What do you feel like 40 image gen is
[08:21] particularly good at compared to some of
[08:23] the other image gen models?
[08:26] It's very good at prompt adherence. So
[08:28] you can do things and I think that's
[08:30] because of the infrastructure underneath
[08:31] it. It's a different infrastructure from
[08:33] the diffusionbased models that preceded
[08:35] it. And BFL, Flux, a bunch of others do
[08:38] this now as well and and it's great. Um,
[08:40] but I think it was just the most
[08:41] productive image model because you could
[08:43] manipulate it in such a fine grained
[08:45] way.
[08:46] Yep. And uh I I remember the biggest
[08:49] improvement when the 40 image gen came
[08:51] out is that it could actually spell
[08:52] things and write letters out. That was
[08:55] magical moment. So, I have to call out
[08:56] that NPR in the in the top corner of
[08:58] this image is actually done correctly.
[09:00] Look, there's there he is with his uh
[09:02] heard.
[09:04] Okay, I'm going to remove the guitar
[09:06] actually so that it is ac cappella cuz I
[09:09] think that might work a little bit
[09:10] better. But look, this is the vibe of
[09:12] Tiny Desk. You know, it's as if you're
[09:14] seeing a photo from the '9s in the Tiny
[09:16] Desk studio. So, I just I love this and
[09:18] I think that we we become so attuned to
[09:21] what's possible we forget that this
[09:22] would be, you know, witchcraft 3 years
[09:25] ago. Witchcraft, right?
[09:27] What is the purpose of this? Are you
[09:30] storyboarding? Are you creating an asset
[09:32] that's going to go into another tool?
[09:33] Why start with on this flow?
[09:35] So, so I'll talk through essentially
[09:36] what I'm going to do. So there's this
[09:38] product called Hedra which is the best
[09:41] way to I think the best way to take a
[09:44] still frame and add custom audio to it.
[09:47] So create a a video that has uh sort of
[09:50] animated from the still frame and
[09:52] includes the audio with the right um lip
[09:54] sync.
[09:56] So and there's a bunch of amazing tools
[09:58] to do this. Sync Labs is one of my
[09:59] absolute favorites as well. Um but Ira
[10:02] is nice because it actually generates
[10:03] the video. So it does the text to video
[10:05] or the the frame to video and then it
[10:07] also adds the audio. So what we're going
[10:08] to essentially do is take this frame.
[10:10] We're going to get the audio from
[10:12] YouTube. We're going to stem separate
[10:14] the audio so we get the audio track we
[10:17] want and then we're going to put them
[10:18] together in Hedra. And that's it.
[10:21] This really is remix culture.
[10:23] It's amazing, isn't it?
[10:24] It is amazing. Okay. So the the asset
[10:26] that you really need to go into this
[10:29] video gen lip sync tool are two things.
[10:32] You need a still image um that can be
[10:35] used to generate the video and then you
[10:37] need some sort of audio to sync this to.
[10:40] So I know we're looking at this music
[10:42] example, but what other examples have
[10:44] you seen people use this kind of
[10:45] workflow for?
[10:46] I think we underestimated how useful it
[10:48] would be to add custom audio to video
[10:51] and there's been a bunch of great, you
[10:53] know, one of the the early examples was
[10:55] taking a speech that somebody was
[10:57] giving. I know Javier Mille did a really
[10:59] famous one and essentially lip-syncing
[11:01] changing the language to English and
[11:03] lip-syncing it that went really viral a
[11:05] couple of years ago. So we've seen and
[11:07] then of course you can imagine a
[11:08] character a photo of a character that
[11:10] you generate and then you want to
[11:11] animate them doing something and
[11:13] speaking at the same time. So you know
[11:15] stories are told this way and these
[11:18] technologies make it really really easy
[11:20] to do so.
[11:21] Oh we got him. Great. Okay. So now he's
[11:24] he's got bad posture, but we'll we'll
[11:26] allow it. It's very grumpy.
[11:27] I think he always did.
[11:29] Yeah,
[11:29] exactly.
[11:30] Okay. So now we've got Kurt. Now what I
[11:34] would do if I didn't actually have a So
[11:37] Tiny Desk has got a really specific
[11:40] acoustic aesthetic, which is it sounds
[11:42] like live instrumentation. So for the
[11:43] Biggie example, I actually found a
[11:46] Biggie cover band playing live in
[11:48] Brooklyn and I pulled that down from
[11:50] YouTube. Um, and then I extracted the
[11:53] actual vocals from the Notorious B.I.
[11:55] and laid them over. But in this case,
[11:57] Nirvana did um a really famous New York
[12:00] City unplugged concert in 93. So there's
[12:03] video of them playing in the way that
[12:05] they would and audio the way that they
[12:06] would um on Tiny. So that is right here.
[12:10] Even in the same cardigan.
[12:12] Even in the same cardigan. Isn't that
[12:14] amazing?
[12:14] Yep.
[12:15] Okay. Okay, so I use this nifty little
[12:16] tool called 4K
[12:20] video downloader, which is slightly
[12:22] sketchy, but that's okay.
[12:23] I love these little utilities that you
[12:26] just, you know, you Google like, "How do
[12:28] I get audio out of YouTube?" And then
[12:29] you look the scariest website possible
[12:32] and you just cross your fingers that
[12:34] your computer won't go up in flames and
[12:36] you download 4K video
[12:38] download. Yes, my Yes, my data is
[12:42] definitely going somewhere sketchy as a
[12:44] result of this. So, for the vibe coders
[12:46] that are listening, I have a request for
[12:47] startup, which is go go find all these
[12:50] uh slightly scary little utils and build
[12:52] me ones that are less sketchy looking.
[12:55] 100% 100%. It's a great idea. Okay. So,
[12:58] now we actually have this. So, we've got
[13:01] the video. Yep. Now, we're going to open
[13:04] Adobe Edition. Okay. So, this is this is
[13:06] a tool that people who have been working
[13:08] in computer audio have been using for 30
[13:10] years plus. It used to be called
[13:12] Cooledit Pro. It's completely beloved
[13:14] and it's very very easy to use which is
[13:16] why so many of us use it. It was of
[13:18] course acquired by Adobe many years ago.
[13:20] It's now called Audition. So I go to
[13:23] Audition and I take this video and I
[13:26] just drop it in. So here we actually
[13:29] have the audio from the video which is
[13:32] really really cool. I'm going to zoom in
[13:34] and I'm going to see the first few
[13:36] seconds of it are blank. So let's just
[13:39] cut that out because we don't want to
[13:40] hear that. Then we're going to zoom out
[13:42] and we're going to take I don't know,
[13:44] let's take 15 seconds. And you can kind
[13:47] of see the audio the video in the bottom
[13:50] left corner there.
[13:50] Oh, got it. So, it's combining the audio
[13:53] and video just so you know exactly what
[13:54] you're syncing up to.
[13:56] Exactly.
[13:57] And I'm going to pretend like you're
[13:59] doing 15 seconds because uh we're doing
[14:02] a very efficient podcast here. But one
[14:04] of the limitations I know having used
[14:07] some of these audio and video gen tools
[14:09] is you're getting small clips right now
[14:12] with what we're we're working with. And
[14:14] so you know what I'm looking forward to
[14:16] is the day where I can have the you know
[14:18] hourong uh Nirvana unplugged tiny desk.
[14:22] But
[14:23] you know do you feel do you ever feel
[14:24] constrained by the kind of length of
[14:26] assets being generated or the um
[14:29] quality? I mean, sort of, but again, I
[14:32] think creativity breeds constraints. So,
[14:34] to not to over rotate on hip-hop, but if
[14:37] you look at the reason that so many
[14:39] samples were used in hip-hop in creative
[14:41] ways in the 80s and '9s was the actual
[14:44] drum machines and samplers had very
[14:46] limited sampling time. So, you could
[14:48] only sample a second of anything. So,
[14:50] you couldn't really sample four bars.
[14:52] And that's why so many producers put
[14:54] tracks together that use these many
[14:56] 1second samples in surprising ways. And
[14:59] once we actually got the technology to
[15:00] sample for more time, we actually got
[15:03] less creativity, I would argue. So I I
[15:06] sort of love the constraints that the
[15:07] technology gives us today.
[15:09] Well, I also love my complaints. I'm
[15:10] like, isn't it annoying that you can't
[15:13] revive Nirvana and overlay their audio
[15:16] and generate a completely fictional
[15:18] concert for longer than 15 seconds in
[15:21] probably under a 30 minute podcast. Like
[15:24] my complaints are so ridiculous because
[15:26] the idea of creating something like this
[15:29] even a year ago sounds so as you said
[15:31] impossible um that we get so spoiled
[15:34] once we get used to these tools.
[15:37] 100% right. No exactly like I mean this
[15:39] stuff we would have called it witchcraft
[15:40] 3 years ago. It would have been um okay
[15:43] now there's two things you can do with
[15:44] this. If we wanted to do an ac cappella
[15:47] only version for example we can use a
[15:49] technology called demox. So, Demox is
[15:52] this amazing technology that allows you
[15:54] to um to extract the vocals from any
[15:59] song. So, here I've forgotten what the
[16:02] actual command line is. So, I just do
[16:05] this. I looked it up in perplexity.
[16:07] What's the actual way to extract two
[16:08] tracks with Demox? We do this demox two
[16:12] stems vocals. And then let's go find the
[16:16] path. Okay. Okay, so this command is
[16:18] going to take that audio file we saved
[16:20] of the first 15 seconds of this concert
[16:24] and it's going to extract the vocals
[16:27] from the instrumentation. So this will
[16:29] be Kurt Cobain singing come as you are
[16:32] ac cappella which as far as I know has
[16:34] never happened which is pretty cool. And
[16:36] then we simply come back here and we say
[16:40] start frame upload an image. Let's use
[16:44] this. Okay, that's our Kurt Cobain audio
[16:47] script. Upload audio and let's use
[16:50] actually the full audio with all the
[16:52] instruments.
[16:54] Add to video and then we just say man
[16:58] singing on tiny desk.
[17:00] What I love about your prompting
[17:03] compared to other how I AI guests is
[17:06] every prompt has been sub six words six
[17:08] words. you're very simple in terms of
[17:11] describing what you want and uh get high
[17:14] quality outputs there. So, I don't know
[17:16] what that says about the uh prompt
[17:18] engineering industrial complex, but
[17:20] proof here that you can use simple
[17:22] prompts to get pretty cool stuff if the
[17:24] tool behind the scenes does does the
[17:26] work for you.
[17:27] I think you've got to give the AI the
[17:29] space as well. You know, if you overly
[17:30] constrain it, it just really struggles
[17:33] to satisfy you. Whereas if you give it
[17:35] less constraints, you know, sometimes it
[17:37] has unexpected results, but often
[17:39] they're unexpected, you know,
[17:40] delightful.
[17:41] Well, that's what I've heard a lot from
[17:43] folks that come from the more creative
[17:44] backgrounds. Um, designers in particular
[17:47] tend to be less precise in their
[17:51] prompting because they want that
[17:52] exploration space that then they can
[17:54] narrow in on. And so I really think it
[17:57] it also comes into play, your prompting
[17:59] technique can come into play based on
[18:01] kind of what profession or what
[18:04] background you're coming from. Engineers
[18:05] want like the most precise. They not
[18:07] only want the code to work, but they
[18:08] want the code to be written exactly how
[18:09] they would write it. And so they're very
[18:10] precise in their prompting. Um where I
[18:13] found designers and and more creative
[18:15] folks uh building different kinds of
[18:16] assets really like that wide open space.
[18:20] Totally. Yes. Exactly. And while we're
[18:23] while we're waiting for this to load, um
[18:25] it might be interesting. I'm just
[18:26] looking at some of the options at the
[18:29] bottom here. So, you have different kind
[18:31] of models that you can use, including
[18:34] one that looks like that they
[18:36] specifically fine-tuned for this,
[18:38] different aspect ratios, orientation,
[18:42] length, probably based on the script.
[18:43] And then, you know, the prompt says
[18:45] prompt your character with emotion and
[18:46] gesture. So, I I am very curious if you
[18:48] put like angsty man singing versus
[18:52] cheerful man singing, if you'd get a
[18:54] different a different version here, even
[18:55] if the audio and video were were the
[18:58] same.
[18:59] It works really well. Absolutely. Yeah.
[19:02] No, this this is such a useful
[19:03] storytelling product. It's it's amazing.
[19:05] And when you combine it with other
[19:06] videog models like V3, you can start to
[19:09] tell real stories, you know.
[19:11] Yeah. Okay, let's check it out.
[19:12] All right.
[19:18] [Music]
[19:25] All right. Pretty cool.
[19:27] It's very good. It's very good.
[19:30] Great.
[19:31] Very satisfying.
[19:32] He even he even manages his mic well,
[19:34] you know, pulls back on some of those
[19:37] notes.
[19:39] That's incredible. And so, you know,
[19:41] could you take this and take different
[19:44] clips of the video and sort of generate
[19:47] a string of these um these videos and
[19:50] maybe put them together in a in a longer
[19:52] form version?
[19:53] 100%. Yeah, I actually was inspired by
[19:55] this. So, I put together a a music
[19:58] video, a little mini music video um for
[20:01] a different Nirvana track. Can I show it
[20:02] to you right now?
[20:03] Yes, we would love to see it.
[20:05] Okay. I used V3 to generate the clips
[20:07] and um and it it turned out great, I
[20:09] think. Hold on one moment.
[20:10] Yeah. And I think if you haven't tried
[20:13] V3, it is pretty
[20:16] incredible. I mean, I can only generate
[20:18] like two and a half videos every day of
[20:21] three, you know, 7-second length or
[20:22] whatever. Um, I'm still uh capped on on
[20:26] usage, but the quality is really good.
[20:28] The physics are really good. It's one of
[20:31] my favorite video models to to play with
[20:33] right now. just as a just as a consumer.
[20:35] It's it's kind of um it's to me my
[20:39] experience with that model has been was
[20:40] very similar to my first experience with
[20:43] MidJourney where just the breadth of
[20:47] things coming out of the model were so
[20:49] incredible to me. So, highly recommend
[20:51] folks give that that model a little
[20:54] spin.
[20:55] It's amazing.
[20:56] Yeah.
[20:56] You've uh you've got to get on Gemini
[20:58] Ultra, Claire, so you have more. I have
[21:01] a um
[21:02] a household Gemini Ultra account. Um but
[21:06] my husband
[21:07] is the is the video gen guys. So he he's
[21:10] up there and by the time I get to it um
[21:13] we we've burned through some tokens. But
[21:16] you know I read all the I spent all the
[21:17] money on cursor. So yeah.
[21:19] Fair fair. I know. My wife for the first
[21:21] time this month was like babe what is
[21:24] cursor?
[21:25] I'm like h don't worry about it.
[21:29] I know all these like little secret AI
[21:31] tools popping up on the credit card.
[21:35] How I AI is now on Lenny's list with my
[21:39] personal selection of the best AI
[21:41] engineering courses on Maven. You can
[21:44] spend months thinking and playing with
[21:46] AI before really integrating it into
[21:48] your workflow or shipping an actual AI
[21:51] feature. If you want to start building,
[21:54] then these hands-on Maven courses are
[21:56] for you. Learn directly from Aishwaria
[21:59] Naresh Reganti, MIT instructor and AI
[22:02] scientist at AWS, or Sander Schuloff,
[22:06] who has authored research with OpenAI,
[22:08] HuggingFace, and Stamford. To pivot into
[22:11] an AI role or successfully lead your
[22:14] company's next AI initiative, visit
[22:17] maven.com/lenny
[22:19] to enroll now. Use code Lenny's for $100
[22:24] off. That's m-aven.com/lenny
[22:30] to get ahead in the AI era and start
[22:33] building.
[22:35] So this is these are all the videos I
[22:37] generated Google Flow. So I was trying
[22:40] to capture like a 1990s high school band
[22:43] auditorium, you know, little dystopian
[22:46] energy. So I generated all these clips
[22:49] in a pretty straightforward way. Okay, I
[22:50] use GPT40 to help me with the prompts
[22:52] cuz as you can see, this is actually the
[22:54] beginning of my generations. This
[22:56] doesn't this is like the complete wrong
[22:57] energy, you know, this I don't know what
[22:59] this is, like early8s, you know, uh,
[23:02] synth pop or something. So, then I went
[23:04] to GP40 and said, "Hey, help me capture
[23:06] like grunge 1990s Seattle, you know,
[23:08] inspired by some of these music videos."
[23:10] And then, as you can see, it gets
[23:12] progressively more like, you know,
[23:14] camcorder and sort of grimy. So, I
[23:17] generated all this stuff and then I
[23:19] threw it together into a music video.
[23:21] Um, and I put the music behind it. I'll
[23:23] show it to you right now.
[23:24] Amazing. So, just restating this for
[23:28] helping you refine your prompts to get
[23:29] the aesthetic right, the phrasing, the
[23:32] prompting right, give you some keywords.
[23:34] Veo to generate these like shorter
[23:36] clips. And then, do you put it together
[23:38] in like Final Cut or something like
[23:40] that?
[23:40] I put it together in Capwing. Capwing is
[23:43] so easy and so useful. Um, highly
[23:45] recommend. Tip top girl, so I use cap
[23:47] cut.
[23:48] Yeah, got to get on capling. All right,
[23:51] let's watch it.
[24:20] Bring friends
[24:26] over.
[24:30] [Music]
[24:38] Hello.
[24:40] [Music]
[24:56] That's it.
[24:57] Okay. You get the patented Clairvo
[24:59] raised hands re reaction on this one.
[25:03] Love it.
[25:03] I'm going to tell you the real truth.
[25:05] Something like this makes me almost want
[25:06] to cry because I really got into
[25:09] technology. I wanted like everybody. I
[25:11] want to like make video games and like
[25:13] make movies and work for Pixar or D like
[25:16] and it always felt so inaccessible to
[25:19] get these like amazing ideas that I had
[25:21] in my head
[25:23] into a thing like could you film it?
[25:26] Could you access the people? Did you
[25:28] have the time? Did you have the music?
[25:30] Did you have the c? you just put
[25:31] together this amazing amazing music
[25:35] video.
[25:36] Thank you.
[25:37] I'm so impressed.
[25:38] Thank you. It was so It was so fun. It
[25:40] was so easy. And also like music videos
[25:43] are a lost art form.
[25:44] Totally.
[25:45] I'm so excited to see, you know,
[25:47] everybody making music videos for all
[25:49] their favorite tracks cuz what a cool
[25:50] way to contribute, you know, and in no
[25:52] way does it actually dilute from the
[25:54] original. I think it's a
[25:55] it's a testament to the original and our
[25:57] appreciation of it. No, it looks like a
[25:58] love letter. And I have to I have to
[26:00] call out when I was watching it, there's
[26:02] a lot of it that I think is incredible.
[26:04] I like how the cameras, you know, like
[26:05] pan and zoom in. Um, the part that
[26:09] really got me was the sequential shots
[26:11] of the teenagers in the hall. And I was
[26:13] like, I cannot believe this is AI
[26:15] generated. It's so high quality.
[26:17] It's so specific in an aesthetic, in a
[26:20] wardrobe, in a motion. And it got me
[26:24] until and again 33 good at physics until
[26:28] there's like a guy with like a pack of
[26:30] camel cigarettes on his arm and like the
[26:32] cigarettes are like halfway coming out.
[26:34] Yes. Yes. Yes. Totally. That's right.
[26:36] Well, the actually the and the other
[26:37] funny artifact is if you look at the end
[26:39] when the band is playing and a bunch of
[26:41] people are jumping out of the crowd.
[26:43] Four people jump out of the crowd at the
[26:44] same time. They look the same and
[26:46] they're making the exact same like, you
[26:48] know, like they look like acrobats at a
[26:51] circus or something. It's like the end
[26:53] of like an 80s TV special where they all
[26:55] jump up with their leg.
[26:56] Totally. Yes. Yes.
[26:58] That's amazing. You have inspired me
[27:01] truly after this podcast. I'm like, what
[27:03] what music video am I going to make?
[27:05] It's so much fun.
[27:06] Do it. Do it.
[27:07] Music. I mean music videos. You could do
[27:09] like fake movie trailers.
[27:12] Yes.
[27:12] All sort all documentaries. cuz I mean I
[27:15] we're doing the fun art, you know, heart
[27:18] heart and soul filling stuff, but I also
[27:20] think the ability to create educational
[27:22] materials that are compelling and
[27:24] interesting
[27:26] um with this technology are also right
[27:28] there.
[27:29] I mean, if you look at fanfiction,
[27:31] fanfiction's enormous because people
[27:33] want to contribute to the things they
[27:34] love and now we get fanfiction for every
[27:37] medium. It's so cool.
[27:39] Okay, sold. All right, that was that was
[27:43] just workflow number one. We're going to
[27:45] go pretty fast through workflow number
[27:47] two, which I think is a little bit more
[27:49] of a practical practical one, but still
[27:52] connected to to the arts. So, walk us
[27:54] through what your second second workflow
[27:56] is.
[27:57] Cool. Yeah, so one of the things that I
[27:58] think is really underhyped,
[27:59] underappreciated, underused is all the
[28:02] multimodal capabilities. And the model
[28:04] that does this really well actually is
[28:05] flash um Gemini flash. So, it's just
[28:08] it's great. It's one of the very few
[28:10] models that can do video analysis and
[28:12] ingestion. It can do all kinds of
[28:14] amazing things and yet I don't see it
[28:16] being used out there a lot. I thought I
[28:19] would use it to create an app that would
[28:21] help me catalog my record collection cuz
[28:23] I've got, you know, like every DJ, I've
[28:25] got so many records and it's such a pain
[28:27] to keep track of them and know which
[28:29] ones I had and which ones I didn't. So,
[28:31] I did a very quick app on Friday that
[28:33] let me take a video of flipping through
[28:35] my record collection and then using
[28:37] Gemini to extract artist names, album
[28:40] names, photos. It's It's really really
[28:42] cool. So, I thought today we could do
[28:44] something similar except for books.
[28:46] H this is amazing. And uh we were
[28:48] talking before we started recording.
[28:50] This is going to help me because over
[28:51] here I have like a 100 books and 100
[28:53] records piled up on shelves um that have
[28:57] definitely not been cataloged. So, I
[29:00] can't wait to see what this looks like.
[29:01] Perfect. I got you. Let's share. So,
[29:04] here we are in Google AI Studio. So, I'm
[29:08] sure folks are familiar with AI Studio,
[29:10] but if you're not, actually, I think
[29:11] it's the best product surface to
[29:13] interact with all the Gemini models. One
[29:15] of the best anyway, um, because it
[29:17] doesn't have all of the kind of overhead
[29:19] and and links and constraints that a lot
[29:22] of the other Gemini products have. This
[29:24] feels like somebody just took a blank
[29:26] piece of paper and brought the best
[29:28] manifestation of the Gemini models
[29:29] forward. So, I really love AI Studio.
[29:32] It's my starting point for all of these
[29:33] things. And then in AI Studio, you can
[29:35] see here you can of course chat, you can
[29:38] stream um with your phone or with your
[29:40] webcam. You can generate media and you
[29:42] can build apps. This is a very good app
[29:45] builder and this is the best way to
[29:46] build off-the-shelf apps I think that
[29:48] integrate with Google models. So here
[29:50] I've I've typed, you know, create an app
[29:51] that takes a video of a person flipping
[29:53] through their book collection and
[29:55] extracts
[29:57] the author and title of every book
[30:00] shown. Then I give it a suggestion for
[30:02] how it could do it, which is you could
[30:04] do this by taking the video and first
[30:05] extracting the frames that show distinct
[30:07] books and then have a vision model
[30:10] analyze those frames to extract the
[30:12] information. Make sure you extract every
[30:16] book shown say sequentially.
[30:20] What I have to call it here is you know
[30:22] what's interesting is people know that
[30:24] these models exist and they generally
[30:26] know some of the capabilities vision you
[30:29] know text to speech or speech to text
[30:32] all this stuff. But what's really hard
[30:35] for people to do and I appreciate you
[30:37] showing us is think of novel ways you
[30:39] can access the abilities of those
[30:42] models. I would have actually I thought
[30:44] you were going to show us like you took
[30:45] a picture of it and you cataloged it.
[30:47] But this idea of a video and then
[30:50] extracting the frames. I just haven't
[30:52] changed my mental model to match these
[30:54] multimodal models in order to take, you
[30:57] know, take advantage of things that can
[30:59] be more efficient, allow you to do
[31:01] things. And so I really think it's great
[31:03] that you're coming to this from how
[31:05] could I solve this with audio, how could
[31:07] I solve this with video, how could I
[31:08] solve this with text and knowing that
[31:11] the models can do kind of the hard work
[31:12] on the back end.
[31:14] Thanks. Yeah, look, I I completely agree
[31:16] and a video is just of course it's so
[31:18] much more rich than image and this is
[31:20] the way that we buil we bring a lot of
[31:22] the outside world online, I think. So
[31:24] I've been really inspired by video. I
[31:26] saw something on Twitter where somebody
[31:27] had set up a mini app that watched him
[31:29] shoot free throws and kept count. You
[31:31] know, you could I mean, there's just so
[31:33] many ways that this will be productive.
[31:34] I'm I'm very passionate about AI for
[31:36] parents, and I've got kind of a neat
[31:38] video idea in there as well. So, um to
[31:41] me, there there's like the sort of
[31:42] skumorphic technologies, which is using
[31:44] the new technology with the old
[31:46] assumptions, and then there's the native
[31:48] ways to use it, and this feels like a
[31:50] very native way to use the models. Well,
[31:52] to connect the two things that you said,
[31:54] the, you know, um, basketball shooting
[31:56] analysis in kids, my husband did upload
[31:59] every single one of our eight-year-olds
[32:02] basketball games to a video analysis to
[32:04] get like each each kid's stats.
[32:06] No way.
[32:08] Shooting percentages, all the They
[32:10] actually don't even keep score at this
[32:12] age. So, he got it to like get the
[32:13] scores.
[32:14] I love that.
[32:15] Yeah. So, I totally love that.
[32:17] Okay. So, now we have an app.
[32:19] Yeah. So, I'm going to take a video here
[32:21] of me just flipping through my stack of
[32:23] books.
[32:32] Okay, I've taken the video.
[32:34] Okay. And that took all of 7 seconds.
[32:37] So,
[32:38] exactly. Yeah. Now, you know, the one
[32:41] edge here that's kind of interesting is
[32:43] this is really it's really easy to get
[32:45] something working, but if you want to
[32:46] publish an app that a lot of other
[32:47] people can use, it it then becomes more
[32:50] work.
[32:50] Yeah.
[32:51] So, I probably it took me 15 minutes to
[32:54] create this for my record collection, at
[32:56] least create the working demo and
[32:57] primitive, but then it took me half a
[32:59] day to get it live. Um, so anybody could
[33:01] use it.
[33:03] And what's interesting about that is I
[33:06] feel like a lot of individuals are just
[33:08] going to build their own tools and
[33:09] presume other people are going to build
[33:11] their own to their own tools. Um and so
[33:13] maybe this will just inspire somebody to
[33:15] build their own record collection
[33:16] extractor which might be faster than
[33:18] trying to f find yours online and and
[33:20] reusing something somebody built.
[33:22] I mean the the era of personal software
[33:24] is upon us, you know.
[33:25] Totally. Okay. So what it's doing taking
[33:28] this video it's going to do frame by
[33:29] frame extraction of video again
[33:32] something that is just so timeconuming
[33:35] and then it's going to use the vision
[33:36] capabilities. What model do you know is
[33:38] behind the the scenes of all this? You
[33:40] say flash.
[33:40] It's flash flash 1.5.
[33:43] Um and I can kind of skip ahead and show
[33:45] you what this looks like. So here is
[33:48] here's one that I built yesterday and
[33:50] which with essentially the exact same
[33:52] prompt.
[33:53] Yep. So, let's let's run it in parallel
[33:54] and see if this one's any happier with
[33:56] us.
[33:57] Okay.
[33:59] And I did notice one was light mode and
[34:01] one was dark mode.
[34:02] Was that just
[34:03] Yeah, this is just some of the
[34:04] randomness of the models. Yeah, exactly.
[34:08] Oh, I do I do have to say I like the um
[34:10] progress indicator of the second one. It
[34:13] told me how many frames it's extracting.
[34:15] Oh, look at this.
[34:16] So, here we go. You know, this is the
[34:19] Chris Dixon book, the Paul Graham book,
[34:22] this very nerdy book that Mark asked me
[34:24] to read when I was hired. Um, this is a
[34:27] really good Thomas Soul history book.
[34:29] Anyways, this is my entire stack of
[34:31] books, every single one of them. You can
[34:32] see a photo. It's extracted the author
[34:34] and and the book name.
[34:36] So, it's and like, you know, this is
[34:39] just a couple of prompts. That's it. And
[34:40] it generated it. So, this is what's
[34:42] possible. And then if you go here to
[34:45] deploy with cloudr run, you get a
[34:48] deployed version of it um that's
[34:50] actually running on the cloud. And now
[34:51] you can send this link to anyone. Now
[34:53] this is going to cost you API credit. So
[34:56] maybe you want to be a little bit
[34:57] deliberate, but you're pretty much ready
[34:59] to go with this really sophisticated
[35:01] video processing app that would have
[35:03] taken, I don't know, a month of time
[35:05] previously.
[35:06] Yeah. Amazing. and so useful because now
[35:10] I can figure out which of these also
[35:11] very nerdy books we we have we've read.
[35:14] I also see some duplicates up there.
[35:16] Totally.
[35:18] It's not perfect. Yeah, exactly. Well,
[35:20] actually in this case it's the photos
[35:22] duplicative, but
[35:23] it detected the Ben book and the Chris
[35:25] book separately. So, but yes,
[35:28] I need this, man. I need this for the
[35:29] pile of kids books I have up in in my
[35:32] kids closet so they even remember what
[35:33] they have.
[35:36] Okay, this is great. Okay. Well, thank
[35:37] you so much for showing us these fun use
[35:40] cases. I have to call out as we hop into
[35:42] our lightning round. One thing I
[35:44] noticed, which is you are using Comet.
[35:48] I am using Comet.
[35:49] Tell me a little bit more about why um
[35:53] that new browser is your browser of
[35:55] choice and what what are you getting out
[35:56] of it?
[35:58] Comet is so good. I mean, I've been
[36:00] skeptical of the new browser thing
[36:02] because it just feels like the ways to
[36:04] improve the browser in the past have
[36:06] been very incremental, ambitious, but
[36:08] there just wasn't that much surface area
[36:10] for new browser features. And now with
[36:12] Comet from Perplexity, it can do a bunch
[36:14] of really incredible things. Um, the my
[36:17] favorite thing that it can do is what's
[36:19] called RPA, right? Which is where the
[36:23] models operate your browser on your
[36:24] behalf. So, you've seen a bunch of
[36:26] examples of this of like, "Hey, go find
[36:28] me a flight and pay for it." Um, which
[36:30] is interesting. The way I've been using
[36:31] it is in my finances. So, I'll go into
[36:34] Robin Hood and I'll say, "Hey, why don't
[36:36] you tell me how my portfolio is
[36:37] performing? Why don't you tell me where
[36:40] I could get stocks that have similar
[36:41] upside at a lower cost basis? What stock
[36:44] should I buy next? Are any of these meme
[36:46] stock?" I mean, you can just go so deep.
[36:49] And look, I could probably figure that
[36:51] out by clicking around the website and
[36:52] downloading the data, but now I don't
[36:54] have to. So, this assistant feature in
[36:57] Comet makes every website dramatically
[37:00] more useful
[37:02] and it's it's been a big unlock for me.
[37:04] I love this whole episode because you've
[37:07] actually shown a couple use cases
[37:08] including talking about p personal
[37:10] finances with Comet that really are
[37:13] consumer use cases. Again, as I said at
[37:15] the beginning, we're doing a lot of like
[37:17] how do you work this inside of an
[37:19] enterprise? How do you write code with
[37:20] it? But I think the real, you know,
[37:23] underappreciated transformation is going
[37:25] to cons come in consumer experience. I
[37:27] think we're so early. I mean, as
[37:29] somebody who does a podcast trying to
[37:31] educate people, I just realized we're so
[37:33] early on consumer adoption of AI. And so
[37:38] I I have a question for you which is if
[37:40] you could get you know like my mom or
[37:43] you know one of my friends that is less
[37:45] you know not in Silicon Valley less in
[37:47] the middle of this in a room and say you
[37:50] know let me show you three things in 15
[37:51] minutes that can totally change how you
[37:53] think about your life um or things that
[37:56] you never knew were possible. What would
[37:57] what would be those things? What are the
[37:59] consumer side things that you're excited
[38:00] about? So, I have kids and parenting's
[38:02] on my mind all the time. And the ways
[38:04] that my kids use models are amazing. Um,
[38:07] so for my four-year-old, Chachi BT reads
[38:10] her a bedtime story, but not just a
[38:13] bedtime story, one that where she can
[38:15] ask infinite questions, you know, so
[38:17] what what what was the king's dragon's
[38:19] name? What color was it? Where did it
[38:21] come from? Did it have any kids? You
[38:23] know, she's really into unicorns and
[38:24] alicorns. Like, tell me a story about an
[38:26] alicorn and a golden egg. And so she can
[38:29] just really interact with the bedtime
[38:31] story and chat GPT is far more patient
[38:35] and creative than we usually are. So
[38:37] that's one way. And look, she can't
[38:39] really use a computer otherwise um other
[38:41] than watching YouTube. And then for my
[38:43] son, he'll set up two figures like
[38:46] Sandman and Spider-Man and then he'll
[38:48] take a photo of them in um in Chachi
[38:51] Beti or one of the other models and say,
[38:53] "Hey, who would win?" And then it'll do
[38:55] this whole, you know, oh, Sandman would
[38:57] win in these conditions and Spider-Man,
[38:59] but maybe Spider-Man does this. So,
[39:01] they're just they're able to kind of
[39:02] play with the technology instead of just
[39:05] being broadcast to from technology,
[39:08] which is really new. That's like the
[39:09] near-term stuff. I think in the longer
[39:11] term, you know, um I think that the
[39:14] models can really help with a lot of
[39:16] social emotional learning. If you look
[39:18] at the classroom, part of it, of course,
[39:20] is academics, but part of it is just
[39:22] teaching children to be, you know, good
[39:24] people for the world. And a lot of that
[39:26] comes in observing how they're sort of
[39:28] behaving and interacting. And we never
[39:30] had a technology that could do that. If
[39:31] your kid went to a great school, there
[39:32] might be a second teacher in the
[39:34] classroom focused on social emotional.
[39:36] So, I think that's how AI shows up in
[39:38] the classroom. It's probably less like
[39:40] homework helpers and assignment
[39:42] generation and more observing the social
[39:45] dynamics in a classroom and helping um
[39:47] kids be better people.
[39:49] Yeah. Well, calling back to what we were
[39:51] saying earlier about trying to identify
[39:53] the AI native way of doing things. I
[39:57] watch my children. So I I I say that my
[39:59] children form my consumer AI thesis for
[40:02] me for me because the other day my um
[40:05] six-year-old was playing Minecraft and
[40:07] he wanted to know how to do a command
[40:09] and he literally went to my purse,
[40:11] picked up my Meta AI glasses, put them
[40:13] on and said, "Hey Meta, how do I
[40:16] transport to the woodland mansion in
[40:18] Minecraft?" And I was like, "Wait, this
[40:20] is like it's not type into chat GBT.
[40:25] It's not even ask Alexa. He took this
[40:27] physical device and put it on his face.
[40:30] Amazing.
[40:30] And asked this personal AI a question.
[40:33] And that just really opened my mind to
[40:36] again I think multimodal is going to
[40:38] change. I think hardware is going to
[40:40] have a real place to play here. And then
[40:43] this like AI native generation is going
[40:46] to think about accessing information and
[40:49] building things in a totally totally
[40:51] different totally different way. So, I
[40:53] am I'm with you on all of that.
[40:56] I love that. Yeah. And it's it's
[40:57] interesting because we have been taught
[40:59] what computers can and can't do.
[41:01] But they haven't been taught any of
[41:02] those things. So, when I generate an
[41:04] image of, you know, a Harry Potter image
[41:06] for my son, I'm like, "Wow, do you see
[41:08] how I just generated that?" He's like,
[41:10] "Dad, of course the computer can do
[41:11] that." So, they just assume that
[41:13] everything's possible and now everything
[41:14] kind of is.
[41:15] Oh my gosh. We had it. So, as I say when
[41:17] I had to walk uphill both ways for my
[41:18] internet like
[41:20] That's right.
[41:22] you and me both.
[41:23] We'll get you out of here. One last
[41:24] question I have to ask. You have had
[41:26] such success with generating these
[41:28] complicated assets. Um, but when AI is
[41:32] not listening to you, when it is giving
[41:33] you really poor results, what is your
[41:37] prompting technique to get it back on
[41:38] track?
[41:39] I mean, I don't know if it's a prompting
[41:40] technique, but it's a sort of it's a
[41:42] it's a mindset. Uh, two things. One is
[41:45] go with it. you know, like let it take
[41:47] you to some strange unexpected places
[41:49] and you might be amazed at the results.
[41:51] I I think the other is just reducing
[41:54] this sunk cost fallacy thing where you
[41:57] know you create a GitHub branch, you try
[41:59] to do something really ambitious, it's
[42:01] just like falling over over and over
[42:03] again. Just abandon the branch and start
[42:05] over because you didn't actually do any
[42:06] work. You feel like you did work because
[42:09] it did work, but that's not you doing
[42:10] work. And and I think being a lot more
[42:12] willing to abandon sort of approaches
[42:15] that aren't working is the sweet spot.
[42:17] I completely agree. Well, thank you so
[42:19] much for showing us all these workflows.
[42:22] It was totally inspiring. I want to get
[42:24] off this podcast so I can go play. So,
[42:26] thank you for making my day and I know
[42:28] everybody's going to love the episode.
[42:30] Thank you, Claire. Super fun.
[42:32] Thanks so much for watching. If you
[42:34] enjoyed this show, please like and
[42:36] subscribe here on YouTube or even
[42:37] better, leave us a comment with your
[42:39] thoughts. You can also find this podcast
[42:41] on Apple Podcasts, Spotify, or your
[42:44] favorite podcast app. Please consider
[42:47] leaving us a rating and review, which
[42:48] will help others find the show. You can
[42:51] see all our episodes and learn more
[42:53] about the show at howiipod.com.
[42:57] See you next time.